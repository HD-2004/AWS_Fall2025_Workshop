[{"uri":"https://hd-2004.github.io/AWS_Fall2025_Workshop/4-eventparticipated/4.5-event5/","title":"Event 5","tags":[],"description":"","content":"Summary Report: “AWS Well-Architected Security Pillar” Event Event Objectives Understand the Security Pillar of the AWS Well-Architected Framework: Gain insights into security best practices and how they apply to cloud architectures, focusing on AWS services and tools.\nLearn core security principles: Explore key principles such as Least Privilege, Zero Trust, and Defense in Depth to implement robust security in cloud environments.\nRecognize cloud security threats in Vietnam: Discuss common security challenges and threats faced by organizations in Vietnam’s cloud environment.\nDive deep into key AWS security pillars: Understand the five key security pillars—Identity \u0026amp; Access Management, Detection, Infrastructure Protection, Data Protection, and Incident Response—and how to implement them effectively.\nKey Highlights Opening \u0026amp; Security Foundation Security Pillar in Well-Architected Framework: Introduction to the importance of the Security Pillar in the AWS Well-Architected Framework and how it ensures a secure architecture.\nCore principles: Focus on Least Privilege, Zero Trust, and Defense in Depth as foundational concepts for designing secure systems.\nShared Responsibility Model: Understanding the division of responsibility between AWS and the customer for security in the cloud.\nTop threats in Vietnam\u0026rsquo;s cloud environment: Identifying the most common security threats faced by organizations operating in Vietnam\u0026rsquo;s cloud environment.\nPillar 1 — Identity \u0026amp; Access Management Modern IAM Architecture: Overview of Identity and Access Management (IAM) in AWS, including users, roles, and policies, and the importance of avoiding long-term credentials.\nIAM Identity Center: Introduction to Single Sign-On (SSO) and permission sets for managing access across AWS accounts.\nSCP \u0026amp; Permission Boundaries: How to use Service Control Policies (SCP) and permission boundaries to manage multi-account environments securely.\nMFA, Credential Rotation, Access Analyzer: Techniques for ensuring secure access through Multi-Factor Authentication (MFA), credential rotation, and access analysis.\nMini Demo: Validate IAM Policy and simulate access to see how IAM works in practice.\nPillar 2 — Detection Detection \u0026amp; Continuous Monitoring: Introduction to key services like CloudTrail, GuardDuty, and Security Hub for monitoring and detecting security events in AWS environments.\nLogging at every layer: Discuss the importance of logging at different layers (e.g., VPC Flow Logs, ALB/S3 logs) for comprehensive security monitoring.\nAlerting \u0026amp; Automation with EventBridge: Learn how to set up EventBridge for automated alerting and incident responses.\nDetection-as-Code: Implementing infrastructure and security rules using Detection-as-Code to automate the detection process.\nPillar 3 — Infrastructure Protection Network \u0026amp; Workload Security: Understanding the importance of VPC segmentation and private vs public placement in securing network traffic.\nSecurity Groups vs NACLs: Discuss the difference between Security Groups and Network Access Control Lists (NACLs) and which model is appropriate for different scenarios.\nWAF + Shield + Network Firewall: Protecting applications and networks using AWS WAF (Web Application Firewall), Shield, and Network Firewall to mitigate threats.\nWorkload Protection: Best practices for securing EC2 instances, ECS, and EKS clusters to ensure workload security.\nPillar 4 — Data Protection Encryption, Keys \u0026amp; Secrets: Introduction to AWS KMS (Key Management Service), including key policies, grants, and key rotation for secure data management.\nEncryption at Rest \u0026amp; in Transit: How to ensure encryption of data at rest (e.g., S3, EBS, RDS) and in transit (e.g., DynamoDB).\nSecrets Management: Using Secrets Manager and Parameter Store for managing sensitive data like passwords, keys, and tokens with automated rotation patterns.\nData Classification \u0026amp; Access Guardrails: Implementing data classification and access controls to ensure data protection.\nPillar 5 — Incident Response IR Playbook \u0026amp; Automation: The lifecycle of an Incident Response (IR) according to AWS, and how to automate the response process using AWS tools.\nIR Playbook: Common scenarios like a compromised IAM key, S3 public exposure, and EC2 malware detection, and how to handle them effectively.\nAuto-response with Lambda/Step Functions: Using AWS Lambda and Step Functions to automate incident response actions and mitigate damage quickly.\nQ\u0026amp;A \u0026amp; Wrap-up Summary of the 5 Pillars: A recap of the five key security pillars and their application in real-world AWS environments.\nCommon Pitfalls \u0026amp; Practices in Vietnamese Enterprises: Discussing common security challenges faced by Vietnamese companies and how to address them with AWS tools.\nSecurity Learning Roadmap: Overview of Security Specialty certification and Solutions Architect – Professional (SA Pro) certification, providing a learning path for security experts.\nWhat I Learned Security Principles \u0026amp; Pillars Core security principles: Understanding the importance of Least Privilege, Zero Trust, and Defense in Depth in the context of cloud security.\nAWS Well-Architected Security Pillars: The significance of each security pillar—Identity \u0026amp; Access Management, Detection, Infrastructure Protection, Data Protection, and Incident Response—in ensuring a secure cloud environment.\nCI/CD Pipeline with AWS: AWS CodeCommit: Best practices for managing source code using AWS’s version control system.\nCodeBuild \u0026amp; CodeDeploy: How to configure automated build and deployment pipelines with AWS tools.\nCodePipeline Automation: The role of AWS CodePipeline in automating the entire software delivery lifecycle.\nIAM \u0026amp; Access Control Modern IAM Architecture: Using IAM effectively to manage access securely, with tools like IAM Identity Center for SSO and MFA for strong authentication.\nMulti-account Security: How to implement SCP and permission boundaries for securing multi-account AWS environments.\nContinuous Monitoring \u0026amp; Detection CloudTrail, GuardDuty, and Security Hub: Setting up CloudTrail for activity tracking, using GuardDuty for threat detection, and aggregating security findings in Security Hub.\nLogging \u0026amp; Automation: Best practices for comprehensive logging and automated alerting with EventBridge.\nMonitoring and Observability: CloudWatch \u0026amp; X-Ray: How to implement full-stack observability in a microservices architecture using AWS CloudWatch and X-Ray for better performance tracking and issue resolution. Data Protection KMS \u0026amp; Encryption: How to implement key management and data encryption to secure sensitive data in the cloud.\nSecrets Management: Best practices for securely managing and rotating secrets with Secrets Manager and Parameter Store.\nIncident Response Automating Incident Response: Using AWS Lambda and Step Functions to automate incident response processes, improving speed and efficiency during security breaches. Application to My Work IAM Management: Implement stronger IAM policies and role-based access control (RBAC) in my organization to ensure secure and granular access management.\nContinuous Monitoring: Set up CloudTrail, GuardDuty, and Security Hub for ongoing monitoring and alerting of potential security threats.\nIncident Response Automation: Automate incident response processes using Lambda and Step Functions to reduce reaction time and improve security efficiency.\nData Encryption: Apply encryption for both data-at-rest and data-in-transit, ensuring that all sensitive data is securely handled.\nMulti-Account Security: Use SCPs and permission boundaries to manage security across multiple AWS accounts.\nExperience at the Event Learning from AWS Experts: The session provided deep insights into cloud security best practices and AWS security tools, which will help me implement effective security measures in my cloud architecture.\nHands-on Demos: I particularly enjoyed the mini demo for validating IAM policies and the incident response demo using Lambda and Step Functions, which provided practical knowledge for my own work.\nNetworking \u0026amp; Knowledge Sharing: The event allowed me to interact with security professionals, expanding my understanding of the challenges specific to cloud security in Vietnam.\nConclusion The “AWS Well-Architected Security Pillar” event was incredibly informative, providing a comprehensive understanding of cloud security best practices. The key takeaways about IAM management, continuous monitoring, data protection, and incident response automation will be directly applicable to securing our AWS environments and improving our incident response times. I now have a clearer roadmap for enhancing security practices in my organization using AWS tools.\n"},{"uri":"https://hd-2004.github.io/AWS_Fall2025_Workshop/4-eventparticipated/4.4-event4/","title":"Event 4","tags":[],"description":"","content":"Summary Report: “DevOps on AWS” Event Event Objectives Provide an overview of AI/ML concepts and their applications within the DevOps environment.\nIntroduce and clarify the DevOps culture and core principles that enhance team collaboration and automation.\nUnderstand key DevOps performance metrics such as DORA metrics, MTTR, and deployment frequency to evaluate and improve processes.\nSupport interns in acquiring essential knowledge and effectively applying it to real projects to complete tasks on schedule and with quality.\nKey Highlights 1. Recap AI/ML Session Summarize basic AI/ML concepts and practical applications in software development and DevOps.\nExplore AI/ML tools and services that support automation and data analytics in DevOps workflows.\n2. DevOps Culture and Principles Introduce DevOps culture: cross-team collaboration, automation, and continuous improvement.\nCore DevOps principles that accelerate development speed and enhance software reliability.\n3. Benefits and Key Metrics Importance of measuring DevOps performance through key metrics:\nDORA metrics: Lead Time, Deployment Frequency, Change Failure Rate, Mean Time to Recovery (MTTR).\nThe meaning of these metrics and how to improve them for optimized development and operations processes.\n4. Practical Applications for Interns Guide on applying DevOps and AI/ML knowledge to projects.\nTips and best practices to help interns complete projects efficiently.\nEnhance teamwork and communication skills in the DevOps environment.\nPillar 1 — AWS DevOps Services - CI/CD Pipeline Present modern IAM architecture including users, roles, policies, emphasizing avoiding long-term credentials to reduce risk.\nIntroduce IAM Identity Center with Single Sign-On (SSO) and permission sets for unified access management across multiple AWS accounts.\nExplain the roles of Service Control Policies (SCP) and permission boundaries in controlling permissions in multi-account environments to enhance security.\nPresent use of Multi-Factor Authentication (MFA), credential rotation, and Access Analyzer to secure and accurately manage access.\nDemo illustrating IAM policy validation and access simulation for practical understanding of IAM in AWS.\nPillar 2 — Infrastructure as Code (IaC) Introduce monitoring and security tools like CloudTrail (API history logging), GuardDuty (threat detection), and Security Hub (aggregated security alerts).\nPractice detailed logging across multiple layers such as VPC Flow Logs, ALB/S3 logs to ensure comprehensive monitoring and early incident detection.\nGuide on using EventBridge to set up automated alerts and trigger response actions upon security events.\nApply Detection-as-Code: build automated detection rules through code to increase automation and efficiency in security management.\nPillar 3 — Container Services on AWS Learn to protect networks and workloads on AWS via VPC segmentation and separation of private/public traffic to minimize public network risks.\nCompare and apply Security Groups and Network ACLs (NACLs) appropriately for traffic control.\nIntroduce AWS services protecting application and network layers like AWS WAF (Web Application Firewall), Shield (DDoS protection), and Network Firewall.\nPresent best security practices for EC2, ECS, and EKS workloads to maintain safe containerized and virtual server environments.\nPillar 4 — Monitoring \u0026amp; Observability Introduce AWS Key Management Service (KMS) for encryption key management, including policies, permissions, and key rotation to protect data.\nPresent encryption techniques for data at rest (S3, EBS, RDS) and in transit (e.g., DynamoDB, other AWS services).\nGuide sensitive data management via Secrets Manager and Parameter Store with automated rotation to reduce risk.\nDiscuss data classification and access control measures to ensure data is accessed only by authorized users and systems.\nPillar 5 — DevOps Best Practices \u0026amp; Case Studies Introduce Incident Response (IR) implementation following AWS best practices combined with automation for fast security incident handling.\nPresent IR playbooks for common scenarios such as leaked IAM keys, public S3 buckets, or malware-infected EC2 instances, including detailed remediation steps.\nDemonstrate AWS Lambda and Step Functions usage for automated incident response to reduce detection and recovery times.\nSummary \u0026amp; Q\u0026amp;A Recap the 5 main AWS security pillars and their practical applications to help enterprises build secure, efficient cloud systems.\nDiscuss common security mistakes and challenges in Vietnamese enterprises and propose AWS-based solutions.\nIntroduce learning paths and certifications in security such as AWS Security Specialty and Solutions Architect – Professional (SA Pro) for skill advancement.\nWhat I Learned DevOps Culture and Principles Deepened understanding of DevOps culture and principles that foster cross-team collaboration, automation, and continuous improvement. Grasped the importance of measuring DevOps effectiveness using key metrics like DORA metrics and MTTR to improve project quality. AI/ML in DevOps Identified AI/ML applications that enable automation and data analysis in DevOps pipelines. Learned how AI/ML supports pipeline optimization and rapid incident detection. AWS DevOps Services and Security Studied modern IAM architecture and secure access management with IAM Identity Center, SCPs, and permission boundaries. Practiced MFA, credential rotation, and Access Analyzer to enhance account and service security. Participated in demos for policy validation and access simulation within AWS IAM. Continuous Monitoring and Detection Applied AWS tools like CloudTrail, GuardDuty, and Security Hub for comprehensive monitoring and security incident detection. Configured multi-layer logging and automated alerting with EventBridge for rapid incident response. Network and Container Management Learned VPC segmentation, and effective use of Security Groups and NACLs to safeguard EC2, ECS, and EKS workloads. Used AWS WAF, Shield, and Network Firewall for layered network and application protection. Data Protection and Encryption Utilized AWS KMS for encryption key management, securing data at rest and in transit. Managed secrets and automated rotation with Secrets Manager and Parameter Store. Applied data classification and strict access controls. Automated Incident Response Built Incident Response playbooks covering detection, analysis, containment, and recovery steps. Automated incident handling using AWS Lambda and Step Functions to minimize damage. Learned to manage typical cases like IAM key leaks, public S3 buckets, and infected EC2 instances. Practical Applications Applied IAM and access management knowledge in real project contexts. Established monitoring and automated alert systems to enhance AWS environment security. Integrated automated incident response to save time and reduce risks. Implemented data encryption and secret management following best security practices. Event Experience Learned directly from AWS experts and engaged in practical demos on security and DevOps. Connected and shared knowledge with the DevOps and security community, expanding professional expertise. Received practical advice and guidance tailored for personal work and projects. Conclusion The “DevOps on AWS” event gave me a comprehensive understanding of DevOps culture, AI/ML applications, and AWS security best practices. Knowledge of IAM, monitoring, data protection, and incident response forms a critical foundation that I will apply in projects, especially supporting interns to complete work efficiently and securely.\n"},{"uri":"https://hd-2004.github.io/AWS_Fall2025_Workshop/4-eventparticipated/4.3-event3/","title":"Event 3","tags":[],"description":"","content":"Summary Report: “AI/ML/GenAI on AWS” Event Event Objectives Explore AWS AI/ML Services: Provide an overview of the AI/ML services available on AWS, helping Attendees understand how to apply these services in real-world projects.\nIntroduction to Generative AI: Focus on leveraging foundational models and Generative AI within AWS to build intelligent applications.\nLive Demo of Generative AI: Guide on building a chatbot using Amazon Bedrock, including techniques like prompt engineering and chain-of-thought reasoning.\nProvide Knowledge on MLOps and SageMaker: Introduce tools that manage the entire machine learning model lifecycle, from data preparation, model training to deployment and monitoring.\nKey Highlights Introduction to AI/ML on AWS: Amazon SageMaker: A comprehensive machine learning platform that helps deploy, train, and manage machine learning models.\nData Preparation and Labeling: Tools to assist with data preparation and labeling for machine learning models.\nMLOps Capabilities: Integrated features in SageMaker to support model operations (MLOps).\nLive Demo – SageMaker Studio Walkthrough: A demonstration of using SageMaker Studio to develop and deploy AI/ML models. Generative AI with Amazon Bedrock: Foundation Models: An introduction to foundational models such as Claude, Llama, Titan, and a guide to choosing the appropriate model.\nPrompt Engineering: Techniques for building effective prompts to optimize the performance of Generative AI models, including Chain-of-Thought reasoning and Few-shot learning.\nRetrieval-Augmented Generation (RAG): Architecture and integration of a knowledge base into the model generation process.\nBedrock Agents: Building multi-step workflows and integrating tools within Amazon Bedrock.\nGuardrails: Safety and content filtering measures when using Generative AI.\nLive Demo – Generative AI Chatbot with Bedrock: A demonstration of building a chatbot using Amazon Bedrock. What I Learned Design Thinking with AI/ML: Amazon SageMaker is the key platform supporting the full model lifecycle, from data preparation to model deployment and monitoring.\nPrompt Engineering: How to optimize Generative AI models using effective prompt-building techniques.\nMLOps: How to manage and automate model training, deployment, and monitoring processes in production environments.\nGenerative AI: Understanding foundational models and how they can create intelligent content, such as chatbots.\nAI/ML Architecture: Foundation Models: The differences between models like Claude, Llama, and Titan, and how to choose the right model based on project requirements.\nRAG (Retrieval-Augmented Generation): Understanding how RAG architecture works and how to integrate knowledge bases into the content creation process.\nBedrock Agents: Guidance on building complex workflows with pre-integrated tools.\nApplication to My Work Apply Amazon SageMaker: Integrate SageMaker tools into the data preparation, model training, and deployment processes for current projects.\nBuild a Generative AI Chatbot: Use Amazon Bedrock to develop chatbots and Generative AI applications for customer service or virtual assistant projects.\nOptimize AI/ML Models with Prompt Engineering: Apply prompt engineering techniques to improve AI models for better results.\nMLOps: Implement MLOps within the company to automate model training and monitoring, improving efficiency and reducing risks.\nExperience at the Event Learning from AWS Experts: The speakers shared in-depth knowledge about AI/ML and Generative AI, which helped me better understand how to use AWS tools for developing models.\nLive Demonstrations: I participated in live demos of SageMaker and Amazon Bedrock, which helped me visualize how to build and deploy AI models in real-world environments.\nNetworking and Interaction: The event allowed me to connect with other Attendees and experts, expanding my professional network and exchanging valuable AI/ML insights.\nConclusion The “AI/ML/GenAI on AWS” event provided me with valuable insights into the latest technologies in AI/ML, particularly around Generative AI and tools like SageMaker and Amazon Bedrock. Techniques like Prompt Engineering and MLOps have shown me the necessary steps to bring AI models from concept to production. I now feel more confident in applying these tools and techniques to real-world projects and advancing our AI/ML capabilities.\n"},{"uri":"https://hd-2004.github.io/AWS_Fall2025_Workshop/4-eventparticipated/4.2-event2/","title":"Event 2","tags":[],"description":"","content":"Summary Report: “AI-Driven Development Life Cycle: Reimagining Software Engineering” Event Event Objectives Explore the AI-Driven Development Life Cycle and how AI is transforming modern software development processes.\nLearn how Amazon Q Developer and Kiro support developers in requirement analysis, code generation, testing, and performance optimization.\nIdentify real-world applications of AI in software development in Vietnam: increased productivity, reduced errors, and improved deployment speed.\nExplore the stages of the AI-driven software development life cycle: Requirement → Design → Coding → Testing → Deployment → Improvement.\nKey Content Opening \u0026amp; Software Engineering Foundation Overview of the transition from Traditional Software Development to AI-Driven Development.\nCore principles of software development when applying AI: transparency, verifiability, and effective collaboration between humans and AI models.\nAI-Augmented Engineer: a model where developers leverage AI to increase productivity and reduce development time.\nChallenges faced by Vietnamese companies in adopting AI for Software Engineering: workflow adaptation, output quality, and integration with existing systems.\nPillar 1 — AI-Driven Development Life Cycle overview and Amazon Q Developer demonstration Detailed introduction of the AI-Driven SDLC, including requirement analysis, design, code generation, automated testing, deployment, and continuous improvement.\nAmazon Q Developer:\nAutomatically generates code from technical specifications. Suggests bug fixes, analyzes logs, and provides improvement recommendations. Generates test cases and documentation automatically. Supports architecture generation and AWS service–related code. Mini Demo: Using Amazon Q Developer to build APIs, write backend code, generate unit tests, and optimize code in real time.\nPillar 2 — Kiro Demonstration Introduction to Kiro – an AI assistant designed for enterprise Software Engineering workflows.\nContinuous analysis and assistance:\nAnalyzes backlog items, requirement documents, and Agile processes. Suggests architectures, technical solutions, and user stories. Optimizes development workflows across Dev – QA – Ops. Workflow automation:\nGenerates Technical Specifications. Recommends codebase improvements and CI/CD process enhancements. Helps track progress and identify bottlenecks. Mini Demo: Using Kiro to analyze requirements and generate design documents and development plans.\nSummary \u0026amp; Q\u0026amp;A Summary of two pillars: how AI supports the entire software development life cycle from analysis to deployment.\nCommon challenges for Vietnamese companies: over-reliance on AI, inconsistent output quality, and the need for developer evaluation skills.\nLearning path: Amazon Q Developer, AWS AI Practitioner, Developer Associate, and specialized courses in GenAI for Software Engineering.\nKey Takeaways Overview of AI-Driven Development Gained a clear understanding of how AI is reshaping the software development process, from requirement analysis to deployment and maintenance. Recognized the “AI-Augmented Engineer” model—where AI assists developers in accelerating development, ensuring code quality, and reducing errors. Amazon Q Developer Learned to use Amazon Q Developer to generate source code from descriptions, helping speed up coding and reduce repetitive work. Understood how Q Developer interprets codebases, proposes refactoring, generates test cases, and analyzes errors. Practiced mini-demos involving generating APIs, handling backend logic, and automatically creating tests using AI. Kiro and Engineering Process Understood how Kiro assists engineering teams in managing the development life cycle: planning, requirement analysis, documentation generation, and architecture recommendations. Observed demos where Kiro analyzes backlogs, generates user stories, and proposes suitable technical solutions. Recognized how AI improves collaboration between Dev – QA – Ops. AI-Driven Software Development Life Cycle Understood all major stages: Requirement → Design → Coding → Testing → Deployment → Improvement and how AI contributes to each. Learned how AI supports automated testing and continuous quality monitoring. Application to Work Applied AI to write code faster, reduce bugs, and increase efficiency during feature development. Used tools like Q Developer to analyze logs, debug, and optimize code performance. Increased productivity in writing technical documentation (design docs, user stories) with AI. Automated repetitive tasks such as refactoring, test writing, and code reviewing. Event Experience Hands-on experience with demos from Amazon Q Developer and Kiro, gaining real-world insights into AI applications. Learned from AWS experts about modern trends in Software Engineering combined with GenAI. Connected with a community of developers applying AI in Vietnam. Conclusion The event “AI-Driven Development Life Cycle: Reimagining Software Engineering” provided deeper insight into applying AI in software development. With Amazon Q Developer and Kiro, developers can accelerate product development, improve code quality, and optimize the entire software development life cycle.\n"},{"uri":"https://hd-2004.github.io/AWS_Fall2025_Workshop/1-worklog/1.1-week1/","title":"Week 1 Worklog","tags":[],"description":"","content":"Week 1 Objectives: Connect and get acquainted with members of First Cloud Journey. Understand the basics of AWS and how to create an AWS Free Tier account. Tasks to be carried out this week: Day Task Start Date Completion Date Reference Material 2 - Get acquainted with FCJ members - Read and take note of internship unit rules and regulations 09/08/2025 09/08/2025 3 - Research the cloud computing technology of Amazon Web Services. + What is Cloud Computing? + what make AWS Different? + How to Start Your Cloud Journey + AWS Global Infrastructure 09/09/2025 09/11/2025 https://cloudjourney.awsstudygroup.com/ 4 - Create AWS Free Tier account - Learn about AWS Console \u0026amp; AWS CLI - Practice: + Create AWS account + Install \u0026amp; configure AWS CLI + How to use AWS CLI 09/10/2025 09/10/2025 https://cloudjourney.awsstudygroup.com/ 5 - Learning AWS Services: + AWS Budgets + AWS Identity and Access Management (IAM) + AWS Support 09/11/2025 09/11/2025 https://cloudjourney.awsstudygroup.com/ 6 - Practice: + Apply Cost Management Services + Access Management + Technical Support 09/11/2025 09/11/2025 https://cloudjourney.awsstudygroup.com/ Week 1 Achievements: Understood what Amazon Web Services (AWS) cloud computing is and grasp the basic foundation:\nCloud Computing is the on-demand delivery of IT resources over the Internet with a pay-as-you-go pricing model. What Makes AWS Different is that it has been the leading cloud infrastructure provider for 15 consecutive years [as of the end of 2025], with a core focus on delivering real customer value in all of its leadership principles. Starting Your Cloud Journey can be done by self-learning through AWS training courses. AWS Global Infrastructure includes Regions → Availability Zones (multiple independent data centers within a Region) → Edge Locations (content delivery closer to users) → Local Zones (resources near major cities) → Wavelength Zones (5G integration) → Outposts (AWS infrastructure deployed in customer data centers). Successfully created and configured an AWS Free Tier account.\nBecame familiar with the AWS Management Console and learned how to find, access, and use services via the web interface.\nInstalled and Using AWS Services::\nCost Management – AWS Budgets Access Management – AWS Identity and Access Management (IAM) Technical Support – AWS Support "},{"uri":"https://hd-2004.github.io/AWS_Fall2025_Workshop/1-worklog/1.2-week2/","title":"Week 2 Worklog","tags":[],"description":"","content":"Week 2 Objectives: Studied AWS Networking Services. Learn the theory and practice of basic EC2. Tasks to be carried out this week: Day Task Start Date Completion Date Reference Material 2 - Study the basic EC2 theory: + Instance types + AMI + EBS - remote SSH into EC2. - Study Elastic IP. - Practice: + Create EC2 instance + Connect SSH emsp; 09/14/2025 09/14/2025 https://000004.awsstudygroup.com/vi 3 - Studied AWS Networking Services + Amazon Virtual Private Cloud ( VPC ); + VPC Peering \u0026amp; Transit Gateway + VPN \u0026amp; Direct Connect + Elastic Load Balancing \u0026amp;emsp 09/15/2025 09/17/2025 https://www.youtube.com/watch?v=O9Ac_vGHquM\u0026list=PLahN4TLWtox2a3vElknwzU_urND8hLn1i\u0026index=25 4 - Studied AWS Networking Services + Amazon Virtual Private Cloud ( VPC ); + VPC Peering \u0026amp; Transit Gateway + VPN \u0026amp; Direct Connect + Elastic Load Balancing \u0026amp;emsp 09/15/2025 09/17/2025 https://000003.awsstudygroup.com/vi 5 - Learn basic EC2: + Instance types + AMI + EBS + \u0026hellip; - SSH connection methods to EC2 - Learn about Elastic IP 09/16/2025 09/17/2025 https://000003.awsstudygroup.com/vi/1-introduce 6 - Group meeting about project ideas and writing worklog 09/18/2025 09/18/2025 Week 2 Achievements: Study the basic EC2 theory::\nInstance types: different virtual hardware configurations provided by AWS to run EC2 instances, each optimized for specific use cases. AMI: a template to launch EC2 instances, including the operating system, software, and volume configuration. Ways to remote SSH into EC2: using an SSH Client with a key pair, EC2 Instance Connect via browser, Session Manager without opening port 22, and PuTTY on Windows with a .ppk file. Elastic IP: a static, public IPv4 address allocated to an AWS account and attachable/detachable to EC2 instances, ensuring a fixed public IP for stable external access. Successfully created and connected to EC2 via SSH.\nStudy AWS Networking Services:\nAmazon Virtual Private Cloud (VPC): a customizable virtual network in AWS Cloud that allows you to create an isolated and secure networking environment. VPC Peering \u0026amp; Transit Gateway: VPC Peering directly connects two VPCs via private IPs (simple but no transitive routing), while Transit Gateway acts as a central hub to connect multiple VPCs and on-premises networks, supporting transitive routing, scalability, and centralized management. VPN \u0026amp; Direct Connect: VPN connects on-premises networks to AWS over the Internet via encrypted tunnels (easy to deploy but Internet-dependent), while Direct Connect provides a dedicated connection with higher stability, bandwidth, and lower latency, though costlier and slower to set up. Elastic Load Balancing (ELB): automatically distributes traffic across multiple resources to improve availability, scalability, and fault tolerance, with ALB (HTTP/HTTPS), NLB (high-performance TCP/UDP), and GWLB (virtual appliances). "},{"uri":"https://hd-2004.github.io/AWS_Fall2025_Workshop/1-worklog/1.3-week3/","title":"Week 3 Worklog","tags":[],"description":"","content":"Week 3 Objectives: Learn about Compute VM services on AWS. Practice with Compute VM services on AWS. Tasks to be carried out this week: Day Task Start Date Completion Date Reference Material 2 - Learn about Compute VM services on AWS. + Amazon Elastic Compute Cloud (EC2) + Amazon Lightsail + Amazon EFS / FSX + AWS Application Migration Service (MGN)regulations 09/15/2025 09/16/2025 https://www.youtube.com/watch?v=-t5h4N6vfBs\u0026list=PLahN4TLWtox2a3vElknwzU_urND8hLn1i\u0026index=72 3 - Learn about Compute VM services on AWS. + Amazon Elastic Compute Cloud (EC2) + Amazon Lightsail + Amazon EFS / FSX + AWS Application Migration Service (MGN)regulations 09/15/2025 09/16/2025 https://www.youtube.com/playlist?list=PLahN4TLWtox2a3vElknwzU_urND8hLn1i 4 - Practice: + Deploy AWS Backup to the System + Using AWS File Storage Gateway + Create SH3 Bucket + Using Autoscaling Group 09/17/2025 09/19/2025 https://000013.awsstudygroup.com/vi 5 - Practice: + Deploy AWS Backup to the System + Using AWS File Storage Gateway + Create SH3 Bucket + Using Autoscaling Group 09/17/2025 09/19/2025 https://000024.awsstudygroup.com 6 - Practice: + Deploy AWS Backup to the System + Using AWS File Storage Gateway + Create SH3 Bucket + Using Autoscaling Group 09/17/2025 09/19/2025 https://000024.awsstudygroup.com Week 3 Achievements: Understood AWS Compute VM services:\nAmazon Elastic Compute Cloud (EC2): Amazon EC2 is similar to a virtual or traditional physical server. It provides fast provisioning, strong resource scalability, and flexibility. Amazon Lightsail: a low-cost compute service (pricing starts at $3.5/month). Each Lightsail instance includes a data transfer allocation (cheaper than EC2), suitable for light workloads, dev/test environments, and applications that do not require high CPU usage continuously for more than 2 hours per day. Amazon EFS / FSx: EFS (Elastic File System): allows creation of NFSv4 network volumes that can be mounted to multiple EC2 instances simultaneously, with storage scaling up to petabytes. EFS only supports Linux and charges based on used storage. It can be mounted to on-premises environments via Direct Connect or VPN. FSx: allows creation of NTFS volumes mountable to multiple EC2 instances using the SMB protocol, supports Windows and Linux, and charges based on used storage. AWS Application Migration Service: used to migrate and replicate servers for building Disaster Recovery sites, continuously copying source physical or virtual servers to EC2 instances in AWS (asynchronously or synchronously). Successfully deployed AWS Backup, File Storage Gateway, and Auto Scaling Group..\n"},{"uri":"https://hd-2004.github.io/AWS_Fall2025_Workshop/1-worklog/1.4-week4/","title":"Week 4 Worklog","tags":[],"description":"","content":"Week 4 Objectives: Discuss and brainstorm project ideas with the group. Learn about storage services on AWS. Tasks to be carried out this week: Day Task Start Date Completion Date Reference Material 2 - Learn about storage services on AWS. + Amazon Simple Storage Service - S3 + Amazon Storage Gateway + Snow Family + Disaster Recovery on AWS emsp; + AWS Backup 09/22/2025 09/23/2025 https://www.youtube.com/watch?v=hsCfP0IxoaM\u0026list=PLahN4TLWtox2a3vElknwzU_urND8hLn1i\u0026index=103 3 - Learn about storage services on AWS. + Amazon Simple Storage Service - S3 + Amazon Storage Gateway + Snow Family + Disaster Recovery on AWS emsp; + AWS Backup 09/22/2025 09/23/2025 https://www.youtube.com/watch?v=hsCfP0IxoaM\u0026list=PLahN4TLWtox2a3vElknwzU_urND8hLn1i\u0026index=103 4 - Perform Lab on AWS storage services. - Practice: + VM Import/Export + Deploy File Storage Gateway 09/24/2025 09/25/2025 https://000014.awsstudygroup.com/vi https://000024.awsstudygroup.com/vi 5 - Perform Lab on AWS storage services. - Practice: + VM Import/Export + Deploy File Storage Gateway 09/24/2025 09/25/2025 https://000014.awsstudygroup.com/vi https://000024.awsstudygroup.com/vi 6 - Group meeting about project ideas and writing worklog 09/26/2025 09/26/2025 Week 4 Achievements: Learned about AWS storage services:\nAmazon Simple Storage Service - S3 Amazon Storage Gateway Snow Familys Disaster Recovery on AWS Successfully imported/exported VM and deployed File Storage Gateway.\nCompleted writing the worklog and finalized the project idea.\n"},{"uri":"https://hd-2004.github.io/AWS_Fall2025_Workshop/1-worklog/1.5-week5/","title":"Week 5 Worklog","tags":[],"description":"","content":"Week 5 Objectives: Learn about security services on AWS. Do the lab. Tasks to be carried out this week: Day Task Start Date Completion Date Reference Material 2 - Learn about security services on AWS. + Shared Responsibility Model - AWS Identity and Access Management + Amazon Cognito + AWS Organization \u0026amp; AWS Identity Center ( SSO ) + AWS KMS 09/29/2025 09/30/2025 https://www.youtube.com/watch?v=tsobAlSg19g\u0026list=PLahN4TLWtox2a3vElknwzU_urND8hLn1i\u0026index=150 3 - Learn about security services on AWS. + Shared Responsibility Model - AWS Identity and Access Management + Amazon Cognito + AWS Organization \u0026amp; AWS Identity Center ( SSO ) + AWS KMS 09/29/2025 09/30/2025 https://www.youtube.com/watch?v=tsobAlSg19g\u0026list=PLahN4TLWtox2a3vElknwzU_urND8hLn1i\u0026index=150 4 Practice: + Compete Lab 18 about AWS Security Hub 10/01/2025 10/01/2025 https://000018.awsstudygroup.com 5 Practice: + Compete lab 22 \u0026amp; 27 about Optimizing EC2 Costs with Lambda và Manage Resources Using Tags and Resource Groups 10/02/2025 10/03/2025 https://000027.awsstudygroup.com https://000022.awsstudygroup.com 6 Practice: + Compete lab 22 \u0026amp; 27 about Optimizing EC2 Costs with Lambda và Manage Resources Using Tags and Resource Groups 10/02/2025 10/03/2025 https://000027.awsstudygroup.com https://000022.awsstudygroup.com Week 5 Achievements: Understand AWS security services:\nShared Responsibility Model: The AWS security model defines the division of responsibilities between AWS and customers in protecting systems and data on the cloud platform. AWS Identity and Access Management (IAM): Manages user identities, roles, and permissions to securely control access to AWS resources. Amazon Cognito: Provides authentication, authorization, and user management for web and mobile applications. AWS Organization \u0026amp; AWS Identity Center (SSO): Enables centralized management of multiple AWS accounts, unified access control, and single sign-on for users across the organization. AWS KMS: Manages encryption keys used to protect data, allowing secure creation, storage, and control of keys. Understand the structure of AWS Security Hub.\nSuccessfully completed Lab 22 \u0026amp; 27.\n"},{"uri":"https://hd-2004.github.io/AWS_Fall2025_Workshop/1-worklog/1.6-week6/","title":"Week 6 Worklog","tags":[],"description":"","content":"Week 6 Objectives: Continue working on the labs from Module 5. Understand basic AWS services and how to use both the AWS Management Console and AWS CLI. Tasks to be carried out this week: Day Task Start Date Completion Date Reference Material 2 Practice: + Do lab 28 about MANAGE ACCESS TO EC2 SERVICES WITH RESOURCE TAGS THROUGH IAM SERVICES 10/06/2025 10/06/2025 https://000028.awsstudygroup.com 3 Practice: + Do lab 30 \u0026amp; 33 about LIMITATION OF USER RIGHTS WITH IAM PERMISSION BOUNDARY and Encrypt at rest with AWS KMS 10/07/2025 10/07/2025 https://000030.awsstudygroup.com https://000033.awsstudygroup.com 4 Practice: + Do lab 44 \u0026amp; 48 about IAM Role \u0026amp; Condition and Granting authorization for an application to access AWS services with an IAM role. 10/08/2025 10/08/2025 https://000044.awsstudygroup.com https://000048.awsstudygroup.com 5 - Learn about Database Services on AWS: + Database Concepts + Amazon RDS + Amazon Aurora + Amazon RedShift + Amazon ElastiCache 10/09/2025 10/09/2025 https://cloudjourney.awsstudygroup.com/ https://www.youtube.com/watch?v=OOD2RwWuLRw\u0026list=PLahN4TLWtox2a3vElknwzU_urND8hLn1i\u0026index=217 6 - Group meeting to draw the system diagram 10/10/2025 10/10/2025 Week 6 Achievements: Learned how to manage access to EC2 services using Resource Tags with AWS IAM. Successfully completed Lab 30 \u0026amp; 33, understanding IAM Permission Boundaries (limiting user permissions) and data encryption at rest using AWS KMS. Gained deeper understanding of IAM Roles \u0026amp; Conditions, and how to grant applications access to AWS services using IAM Roles. Acquired additional knowledge about AWS Database Services: Database Concepts Amazon RDS: A managed relational database service that supports multiple database engines such as MySQL, PostgreSQL, MariaDB, Oracle, and SQL Server. It automates tasks like backup, patching, and scaling. Amazon Aurora: A high-performance, fully managed relational database compatible with MySQL and PostgreSQL, designed for scalability and high availability. Amazon RedShift: A fully managed data warehouse service optimized for large-scale data analysis (OLAP) and big data analytics. Amazon ElastiCache: A managed in-memory caching service supporting Redis and Memcached, improving application performance by reducing database load and query latency. "},{"uri":"https://hd-2004.github.io/AWS_Fall2025_Workshop/1-worklog/1.7-week7/","title":"Week 7 Worklog","tags":[],"description":"","content":"Week 7 Objectives: Complete the labs from module 6. Understand basic AWS services and how to use both the console \u0026amp; CLI. Tasks to be carried out this week: Day Task Start Date Completion Date Reference Material 2 Lab practice for module 6: + Complete Lab 5 on Amazon Relational Database Service (Amazon RDS) + Complete Lab 43 on schema conversion and database migration 10/13/2025 10/14/2025 https://000005.awsstudygroup.com https://000043.awsstudygroup.com 3 Lab practice for module 6: + Complete Lab 5 on Amazon Relational Database Service (Amazon RDS) + Complete Lab 43 on schema conversion and database migration 10/13/2025 10/14/2025 https://000005.awsstudygroup.com https://000043.awsstudygroup.com 4 - Research and work on the group project 10/15/2025 10/17/2025 5 - Research and work on the group project 10/15/2025 10/17/2025 6 - Research and work on the group project 10/15/2025 10/17/2025 Week 7 Achievements: Successfully completed Lab for module 6\nMaking good progress on researching and working on the group project\n"},{"uri":"https://hd-2004.github.io/AWS_Fall2025_Workshop/1-worklog/1.8-week8/","title":"Week 8 Worklog","tags":[],"description":"","content":"Week 8 Objectives: Review lessons to prepare for the midterm exam Tasks to be carried out this week: Day Task Start Date Completion Date Reference Material 2 - Review knowledge to prepare for the midterm exam 10/20/2025 10/24/2025 3 - Review knowledge to prepare for the midterm exam 10/20/2025 10/24/2025 4 - Review knowledge to prepare for the midterm exam 10/20/2025 10/24/2025 5 - Review knowledge to prepare for the midterm exam 10/20/2025 10/24/2025 6 - Review knowledge to prepare for the midterm exam 10/20/2025 10/24/2025 Week 8 Achievements: Still in the review process. "},{"uri":"https://hd-2004.github.io/AWS_Fall2025_Workshop/1-worklog/1.9-week9/","title":"Week 9 Worklog","tags":[],"description":"","content":"Week 9 Objectives: Review lessons and take the midterm exam. Tasks to be carried out this week: Day Task Start Date Completion Date Reference Material 2 - Review knowledge to prepare for the midterm exam 10/27/2025 10/29/2025 3 - Review knowledge to prepare for the midterm exam 10/27/2025 10/29/2025 4 - Review knowledge to prepare for the midterm exam 10/27/2025 10/29/2025 5 - Take the midterm exam 10/30/2025 10/30/2025 6 - Research and work on the group project 10/31/2025 10/31/2025 Week 9 Achievements: Finished reviewing and completed the midterm exam. "},{"uri":"https://hd-2004.github.io/AWS_Fall2025_Workshop/1-worklog/","title":"Worklog","tags":[],"description":"","content":"On this page, you will need to introduce your worklog. How did you complete it? How many weeks did you take to complete the program? What did you do in those weeks?\nTypically, and as a standard, a worklog is carried out over about 3 months (throughout the internship period) with weekly contents as follows:\nWeek 1: Getting familiar with AWS and basic AWS services\nWeek 2: Learned the basics of EC2, successfully deployed and connected instances, and understood AWS networking services.\nWeek 3: Understood AWS Compute VM services and successfully deployed AWS Backup, File Storage Gateway, S3 bucket, and Autoscaling Group.\nWeek 4: Learned AWS storage services, practiced VM Import/Export and deployed File Storage Gateway, completed the worklog, and finalized the project idea.\nWeek 5: Understood AWS security services and completed labs on EC2 cost optimization and resource management.\nWeek 6: Completed IAM labs, gained basic knowledge of AWS KMS, and explored AWS database services.\nWeek 7: Completed module 6 labs (RDS and database migration) and made progress on researching and implementing the group project.\nWeek 8: In the process of reviewing for the midterm exam.\nWeek 9: Reviewed and completed the midterm exam.\nWeek 10: Continued progress in researching and implementing the group project.\nWeek 11: Continued progress in researching and implementing the group project.\nWeek 12: Continued progress in researching and implementing the group project.\n"},{"uri":"https://hd-2004.github.io/AWS_Fall2025_Workshop/3-blogstranslated/3.1-blog1/","title":"Blog 1","tags":[],"description":"","content":"Visually build telephony applications with AWS Step Functions Author: Reynaldo Hidalgo | March 17, 2025 | in Amazon Chime SDK, Application Services, Architecture, AWS Step Functions, Best Practices, Business Productivity, Contact Center, Technical How-to Developers face several challenges when building telephony applications: managing unpredictable user responses, handling disconnects, processing incorrect input data, and resolving errors. These challenges extend development cycles and result in unstable applications that fail to meet user expectations.\nThis article explains how Amazon Web Services (AWS) Step Functions, combined with the Amazon Chime SDK Public Switched Telephone Network (PSTN) audio service, provides a solution to overcome these challenges.\nSolution Overview To illustrate the solution, we built a sample telephony application that allows business owners to manage customer calls through a dedicated business phone number. This solution helps small business owners separate personal and business communications while managing all calls from their existing phone.\nThe beta version of this sample application offers six core call flows:\nDuring business hours: Forward incoming customer calls to the business owner After business hours: Allow customers to leave voicemail Retrieve voicemail: Allow the business owner to access customer voicemail Business caller ID: Allow the business owner to call customers using the business phone number Schedule a call: Allow the business owner to schedule calls with customers for later in the day Automated calling: Automatically initiate scheduled calls between the business owner and the customer Using Workflow Studio, we built a Step Functions workflow that handles all six call flows and manages unexpected scenarios.\nHow it Works AWS Step Functions enables flexible, visual workflow design through pre-built components and error-handling rules. This creates workflows that include event-driven states, allowing the input, processing, and output of JavaScript Object Notation (JSON) formatted messages. The PSTN audio service streamlines telephony applications through a serverless approach using a request/response programming model. This service invokes AWS Lambda functions with Events and waits for Action responses, both of which are in a pre-defined JSON format. This common JSON format allows seamless integration between the PSTN audio service and Step Functions, leading us to design a serverless architecture (Figure 2) that allows two-way JSON message exchange between the two services.\nKey Components: eventRouter: A Lambda function that manages the JSON message exchange appWorkflow: Step functions that implement the call flow logic actionsQueue: An Amazon Simple Queue Service (Amazon SQS) queue that stores response actions Architecture Flow: The PSTN audio service receives an incoming call The service sends a NEW_INBOUND_CALL event to the eventRouter The eventRouter creates the actionsQueue The eventRouter executes the appWorkflow asynchronously with event data The eventRouter starts sending long notifications from the actionsQueue, waiting for the next action notification The appWorkflow processes the event data in JSON format, calculating the next action The appWorkflow queues the next action using Amazon SQS SendMessage API with the Wait for Callback integration model, using a task token to pause the workflow until the next event call is received The eventRouter retrieves and removes the action from the actionsQueue The eventRouter returns the action to the PSTN audio service Observations: The eventRouter logic is generic and independent of specific calls and other Step Function workflows The eventRouter queries an environment variable to determine which workflow to call. The actionQueue and appWorkflow instances persist throughout each call. The eventRouter is responsible for creating and deleting each actionsQueue. The appWorkflow instances are created by the eventRouter at the start of each call. The appWorkflow instances complete execution when all call participants hang up. Building Your Telephony Application: Prerequisites Familiarity with developing workflows in Step Functions Workflow Studio Access to the AWS Management Console Deployment Guide Create a dedicated Step Functions workflow for each telephony application\nDesign and deploy the workflow using Workflow Studio\nUse the Standard workflow type to support long-duration calls\nUpdate the environment variable \u0026ldquo;CallFlowsDIDMap\u0026rdquo; in the eventRouter Lambda function to map phone numbers to their corresponding workflows’ Amazon Resource Names (ARN)\nSet the workflow variable in the \u0026ldquo;Initialization\u0026rdquo; state tab (Figure 3). The eventRouter function automatically sets the \u0026ldquo;QueueUrl\u0026rdquo;, and adding other variables here eliminates the need for external storage\nConfigure Choice state rules to route calls based on conditions. Rules one to three (Figure 4) manage call routing based on inbound/outbound direction and the caller/customer identity, while the default rule handles unexpected scenarios.\nConfigure the SQS state: SendMessage (Figure 5) to guide the PSTN audio service’s next action by: Formatting the message content to match supported actions for the PSTN audio service Setting TransactionAttributes to pass the “WaitToken” and “QueueUrl” values throughout the call Triggering the \u0026ldquo;Wait for Callback with Task Token\u0026rdquo; integration model\nLeverage AWS service integration states to interact with other AWS services directly from the workflow.\nExample: Use the DynamoDB PutItem state (Figure 6) to store Amazon Simple Storage Service (Amazon S3) recorded files, including the name and bucket key, in DynamoDB.\nUse JSONata expressions (Figure 7) to minimize the number of Lambda functions. Example: For Amazon EventBridge scheduling, calculate time expressions using the JSONata functions [$fromMillis(), $millis(), number()] and concatenate strings to handle customer call scheduling.\nKey Benefits This approach to building telephony applications offers several advantages:\nWorkflow designer based on a visual process Self-recording call flow logic Version management and publishing Native integration with AWS Services Visual logs and debugging for each call Automatic scaling Pay-as-you-go pricing Solution Deployment The following steps allow you to deploy the sample telephony application along with a serverless architecture (Figure 2).\nPrerequisites Access to the AWS Management Console Install Node.js and npm Install and configure the AWS Command Line Interface (AWS CLI) Deployment Guide: The AWS Cloud Development Kit (CDK) project in AWS’s GitHub repository will deploy the following resources:\nphoneNumberBusiness – The business phone number provided for the sample application sipMediaApp – The SIP media application that routes calls to - lambdaProcessPSTNAudioServiceCalls sipRule – SIP rule that forwards calls from phoneNumberBusiness to sipMediaApp stepfunctionBusinessProxyWorkflow – Step Functions workflow for the sample app roleStepfuntionBusinessProxyWorkflow – IAM role for - stepfunctionBusinessProxyWorkflow lambdaProcessPSTNAudioServiceCalls – Lambda function to process the call roleLambdaProcessPSTNAudioServiceCalls – IAM role for - lambdaProcessPSTNAudioServiceCalls dynamoDBTableBusinessVoicemails – DynamoDB table to store customer voicemail s3BucketApp – S3 bucket to store system recordings and customer voicemails s3BucketPolicy – IAM policy granting PSTN audio service access to - s3BucketApp lambdaOutboundCall – Lambda function to schedule calls with customers roleLambdaOutboundCall – IAM role for lambdaOutboundCall roleEventBridgeLambdaCall – IAM role that allows EventBridge service to invoke lambdaOutboundCall Follow these steps to deploy the CDK stack: Clone the repository git clone https://github.com/aws-samples/amazon-chime-sdk-visual-media-applications cd amazon-chime-sdk-visual-media-applications npm install Bootstrap the stack #default AWS CLI credentials are used, otherwise use the –-profile parameter #provide the \u0026lt;account-id\u0026gt; and \u0026lt;region\u0026gt; to deploy this stack cdk bootstrap aws://\u0026lt;account-id\u0026gt;/\u0026lt;region\u0026gt; Deploy the stack #default AWS CLI credentials are used, otherwise use the –-profile parameter #personalNumber: the personal phone number of the business owner in E.164 format #businessAreaCode: the United States area code used to provision the business number cdk deploy –-context personalNumber=+1NPAXXXXXXX –-context businessAreaCode=NPA Call the provided phone number to test the sample application. Optionally, edit the workflow to update the business name and business hours in the \u0026ldquo;Initialization\u0026rdquo; Task state in the Variables tab.\nClean Up Resources To clean up this test, run:\ncdk destroy This blog demonstrates how combining AWS Step Functions and the Amazon Chime SDK’s PSTN audio service simplifies the development of reliable telephony applications through visual workflow design and error management. We provided a sample application, implementing six core features for business telephony, illustrating how this solution effectively manages multiple conditional paths and exceptions such as disconnects and invalid inputs. The serverless architecture created enables seamless integration between the two services via JSON-based communication, providing automatic scaling\nTAGS: best practices, customer engagement, Messaging and Targeting\nAbout the Author\nReynaldo Hidalgo\nReynaldo is a Cloud Solutions Architect at AWS, with over 20 years of experience in software, database, and business intelligence decryption, as well as call center/telephony infrastructure and applications. He is also a co-founder of PrimeVoiX, a startup focused on international contact center solutions.\n"},{"uri":"https://hd-2004.github.io/AWS_Fall2025_Workshop/3-blogstranslated/3.2-blog2/","title":"Blog 2","tags":[],"description":"","content":"Visualize and gain insights into your VPC with Amazon Q in Amazon QuickSight Authors: Diego Hernandez, Opeoluwa Victor Babasanmi, and Rashmiman Ray | February 27, 2025 | in Amazon Q, Amazon QuickSight, Amazon VPC, Analytics, Networking \u0026amp; Content Delivery\nIntroduction AWS services generate a wealth of data in the form of logs and metrics, enabling you to build comprehensive dashboards to extract valuable insights, including the ability to observe detailed connection patterns in a Virtual Private Cloud (VPC). This article illustrates how Amazon QuickSight and Amazon Q in QuickSight allow you to visualize data from any source. We will focus on visualizing connection patterns in a VPC, highlighting the benefits of QuickSight for users at various levels of technical expertise.\nAmazon QuickSight is a fully managed, scalable cloud data analytics and business intelligence (BI) service that enables users to create and share interactive dashboards, analyze data, and gain insights through visualizations. On April 30, 2024, AWS announced the general availability (GA) of Amazon Q in QuickSight, a feature that extends QuickSight by integrating generative AI to visualize data via natural language queries.\nIn the following sections, we will explore two use cases — where data is collected from different sources, processed and enriched, and then visualized using QuickSight. These examples demonstrate how Amazon Q can quickly generate visual charts from natural language queries, showcasing the convenience and efficiency of using QuickSight and Q to build exploratory and analytical dashboards.\nOverview This article introduces two use cases that illustrate how to visualize VPC Flow Logs data with Amazon Q in QuickSight. These examples are flexible and can be applied regardless of the data source or solution used to collect and enrich the data. While these cases are tailored for the article, you can refer to and customize them for other practical scenarios in your system.\nUse Case #1: VPC Flow Logs enriched with Security Group and Cost Tag information VPC Flow Logs generate records with predefined fields. AWS Lambda can augment these logs by adding additional fields, creating a custom view of VPC traffic flows. In the architecture shown in Figure 1, VPC Flow Logs are created and sent to Amazon Data Firehose. The logs are then forwarded to Lambda for enrichment. Lambda adds attributes to the flow, such as the security group and cost tags for Amazon Elastic Compute Cloud (Amazon EC2) instances in the VPC Flow Log, before delivering them to Amazon S3. The AWS Glue Catalog stores table definitions for the enriched VPC Flow Logs, allowing Amazon Athena to query the data efficiently. Finally, QuickSight visualizes the data from Athena. The data flow is as follows:\nDetailed Data Flow:\nEnable VPC Flow Logs on the existing VPC via an AWS CloudFormation template. Log records are sent to Amazon Data Firehose. Data Firehose is configured to forward metrics and logs to Amazon CloudWatch for troubleshooting. Data Firehose triggers Lambda, which enriches the Flow Logs with attributes such as security groups and cost tags for Amazon EC2. Lambda is also configured to send metrics and logs to CloudWatch for monitoring and troubleshooting. Enriched VPC Flow Logs data is stored in an S3 bucket. AWS Glue scans the S3 bucket to create a data catalog for querying. Amazon Athena queries the cataloged data from AWS Glue. Amazon QuickSight uses Athena and Amazon S3 as the data source for the dataset. A blank analysis is pre-configured to start visualizing the Flow Logs. Amazon Q is enabled in the analysis through a Q topic. Q topic allows QuickSight to interpret natural language queries, making it easier to auto-generate charts and visualize the data. Use Case #2: VPC Flow Logs and Amazon Route 53 Resolver Logs VPC Flow Logs and Amazon Route 53 Resolver Logs provide complementary information about network traffic within your VPC.\nIn the architecture shown in Figure 2, VPC Flow Logs capture detailed information about traffic between different IP addresses within the VPC but do not include domain names. When combined with Route 53 Resolver Logs, you can gain a more comprehensive view of network traffic within the VPC, including the IP addresses and the domain names that servers or applications are contacting.\nVPC Flow Logs are enabled on the existing VPC via CloudFormation template. VPC Flow Logs are sent to Amazon S3 for storage. Route 53 Resolver Logs are also created and linked to the corresponding VPC where VPC Flow Logs are active. Route 53 Resolver Logs are sent to Amazon S3. AWS Lambda is used to run Athena queries, combining data from VPC Flow Logs and Route 53 Resolver Logs. Both types of logs are merged based on IP addresses on a daily basis. This process links network traffic in VPC Flow Logs to domain names. The analysis is updated daily, and any changes in the mapping between domain and IP are automatically reflected. Amazon QuickSight uses Athena and Amazon S3 as the data source for the dataset. A blank analysis is pre-configured to start visualizing the VPC Flow Logs data. Amazon Q is enabled in the analysis through a Q topic. Q topic allows QuickSight to interpret and process natural language queries, making it easier to generate charts and visualize the data. This article provides a detailed walkthrough for the first use case. To learn more about both use cases, including the deployment steps and related CloudFormation templates, you can access the AWS Samples repository on GitHub.\nPrerequisites: Before starting, follow these steps:\nDeploy one of the CloudFormation templates provided in the aws-samples GitHub repository. Each example in this repository has its own detailed documentation stored directly on the aws-samples GitHub. Ensure that your QuickSight user account has a PRO role to access the Amazon Q features demonstrated in this article. This article provides two demonstration examples. In the future, AWS may add additional examples to the aws-samples repository. The walkthrough steps in this article can be applied to all examples provided. Detailed Guide: Extract insights from your data with Q in QuickSight Review the Q topics configured in the deployed CloudFormation template. Q topics simplify data visualization queries by setting up synonyms for fields in natural language queries. Query Q in QuickSight to explore insights from your VPC Flow Log data. Review Q Topics in QuickSight In this section, we will review the Q topic automatically deployed through the CloudFormation template.\nOpen the AWS Console and navigate to Amazon QuickSight. In the left navigation pane, select Topics. Select the corresponding topic name. Select the tabs at the top, navigate to Data, then to DATA FIELDS. QuickSight automatically populates the data fields from the dataset into the data fields. It will also populate synonyms for corresponding fields where possible. In this example, synonyms are defined in the CloudFormation template. Synonyms help Amazon Q map your queries to fields in the dataset. You can further customize these data fields to match your organization’s terminology, which will provide more meaningful and accurate responses from Amazon Q. Refresh the Q topic indexes to reflect these changes in future analyses.\nAnother useful customization is Field Value Synonyms. This feature allows you to add synonyms for values in your VPC Flow Logs data. In the following example, synonyms were added for IP protocol numbers. This enables Q to interpret VPC Flow Logs queries using terms like icmp, tcp, udp.\nQuery Q for insights from VPC Flow Log data We ask Q in QuickSight a question related to the VPC Flow Log data. Select \u0026ldquo;Ask a question\u0026rdquo; at the top center of the QuickSight screen.\nAs shown in Figure 7 below, we asked Amazon Q: \u0026ldquo;Which cost tags have the most megabytes in October 2024 by hour?\u0026rdquo; Amazon Q processed our question and, using the defined synonyms, interpreted the question as \u0026ldquo;Total megabytes per hour Starting Date and Cost Tag for Starting Date in October 2024.\u0026rdquo; Amazon Q provided an answer to the question, supported by detailed visual data to illustrate the answer.\nGuide: Fill QuickSight Analysis with Amazon Q Link the Q topic to QuickSight Analysis Fill QuickSight Analysis by adding visuals generated by Amazon Q. This article provides a few example queries.\nLink QuickSight Analysis to the Q topic After reviewing the Q topics, we will explore the Analysis. Analysis is a collection of visuals, interactive tables, and insights into data created and organized across one or more sheets to effectively explore and present the data.\nWe create the first visuals in Analysis. The CloudFormation template has created a blank analysis linked to the dataset containing VPC Flow Logs data from Athena. Select the name of the analysis to get started, as shown in Figure 8.\nTo run a query on Amazon Q, link the Q topic to the analysis. In the top center toolbar, select the three vertical dots (⋮) next to Build visual and select Link Topic from the menu. Enable the Link Topic to Build Visual and Ask \u0026amp; Answer options, then choose the topic deployed by CloudFormation from the dropdown menu. Select APPLY CHANGES, as illustrated in Figure 9.\nCreate content for QuickSight Analysis In the top center toolbar, select Build visual. The Build a visual panel will appear on the right-hand side. Start by typing your first natural language query and select BUILD. We’ve provided four sample questions to get you started for the first use case. QuickSight documentation offers the types of questions that Q supports along with more sample queries. QuickSight Enterprise Edition is a prerequisite to ask questions using Amazon Q.\nSample questions:\nDisplay top source and destination IPs by gigabyte Display top source IPs and internet gateway paths by gigabyte Display top security groups by megabyte Display megabytes for the day starting in May, by hour The second use case has its own set of sample questions recorded in the GitHub repository.\nAmazon Q interprets the questions and generates queries based on fields from the dataset and synonyms defined in the linked topic. Amazon Q also selects a visualization type, which can be changed. In the top-right corner of the visual, select the bar chart image and choose the type of visual that represents the data most effectively for you. Select ADD TO ANALYSIS, as shown in Figure 10.\nThe following information table is populated with visuals from the four sample questions, as shown in Figure 11.\nConclusion In this article, you learned how to use Amazon QuickSight to visualize data from multiple data sources using natural language queries. Amazon Q in QuickSight helps democratize data access, empowering everyone in your organization to make data-driven decisions. Organizing data into visual topics and enabling natural language querying allows Amazon Q users to better understand VPC flows. To help your team get started with QuickSight, we recommend reviewing the following guides: What is Amazon QuickSight Q and Best Practices for QuickSight Q Authors.\nAbout the Authors: Rashmiman Ray Rashmiman is a Technical Account Manager at AWS, based in New Jersey. He works with AWS Enterprise customers, providing technical guidance and best practice recommendations to help them succeed on the cloud platform. Outside of work, he enjoys hiking, playing cricket, and cooking delicious Indian dishes.\nOpeoluwa Victor Babasanmi Victor is a Senior Network Specialist Solutions Architect at AWS. He focuses on providing customers with technical guidance on planning and building solutions based on best practices, while also proactively maintaining stable operations for their AWS environments. When he’s not supporting customers, you can find him playing soccer, working out, or looking for a new adventure somewhere.\nDiego Hernandez Diego Hernandez is a Technical Account Manager at AWS, based in Canada. Diego\u0026rsquo;s passion is anything related to networking and relationships. In his free time, Diego enjoys spending time with his family and skiing.\n"},{"uri":"https://hd-2004.github.io/AWS_Fall2025_Workshop/3-blogstranslated/3.3-blog3/","title":"Blog 3","tags":[],"description":"","content":"Validate Your Lambda Runtime with CloudFormation Lambda Hooks (Validate your Lambda runtime environment using Lambda Hooks in CloudFormation.)\nAuthor: Matteo Luigi Restelli and Stella Hie | April 02, 2025 | In AWS CloudFormation, AWS Lambda, Configuration, compliance, and auditing, DevOps, Intermediate (200), Learning Levels, Management \u0026amp; Governance, Management Tools\nIntroduction: This article illustrates how to leverage AWS CloudFormation Lambda Hooks to enforce compliance rules during resource initialization, allowing you to evaluate and validate the configuration of Lambda functions against custom policies before deployment. Typically, these policies affect how software is built, such as limiting language versions or runtimes. A prime example is applying these policies to AWS Lambda — the serverless compute service that allows code to run without the need to provide or manage servers. While AWS Lambda automatically handles the deprecation of runtimes, preventing you from deploying unsupported runtimes, some organizations still need to define and enforce their own compliance rules — rules that aren\u0026rsquo;t directly related to deprecating a specific language version.\nIntroduction to Lambda Hooks: AWS CloudFormation Lambda Hooks are a powerful feature that allows developers to evaluate CloudFormation and AWS Cloud Control API operations via custom code deployed as Lambda functions. This capability enables proactive evaluation of resource configurations before initialization, thereby enhancing security, compliance, and operational efficiency.\nLambda Hooks provide a mechanism to intercept and evaluate a wide range of operations within AWS CloudFormation, including resource operations, stack operations, and change sets (this feature can also be used with the Cloud Control API, but this article focuses on CloudFormation). When you trigger a Lambda Hook, CloudFormation registers an entry in your account\u0026rsquo;s registry as a Private Hook, allowing you to configure the hook for specific AWS accounts and regions. During the Lambda Hooks configuration process, you can specify one or more Lambda functions to be invoked during the evaluation process. These functions can reside in the same account and region as the Hook, or in a different account (as long as the proper access permissions are set up). The evaluation occurs at specific points in the CloudFormation Stack lifecycle — such as during stack creation, update, or deletion. At these points, the configured Lambda functions are called to evaluate the proposed changes based on the compliance rules you have defined. Based on the evaluation results, the hook can either block the operation or trigger a warning, allowing the operation to proceed.\nLambda Hooks allow for proactive evaluation of resources before they are initialized by CloudFormation, creating a governance layer. This means non-compliant resources are detected and blocked before deployment, rather than needing to be fixed afterward. By leveraging Lambda Hooks, organizations can automate and standardize compliance checks across all AWS accounts and regions, ensuring consistency and reducing the burden of manual management.\nSolution Overview: The following sections demonstrate a practical use case of AWS CloudFormation Lambda Hooks, focusing on enforcing compliance rules for AWS Lambda runtimes.\nIntroducing AnyCompany — a pioneering company with strict compliance rules in software development. Among these rules is a strict policy on the use of specific AWS Lambda runtimes.\nAs AnyCompany continues to transition to a serverless architecture, they face a challenge: how to prevent the deployment of Lambda functions using non-compliant runtimes. Since this company uses AWS CloudFormation to deploy Lambda functions, AnyCompany seeks to leverage the power of AWS CloudFormation Lambda Hooks to solve this problem.\nWe will explore the setup process, demonstrate how the Hook works, and discuss the broader implications of maintaining compliance in a dynamic cloud computing environment.\nArchitecture: The architecture diagram below illustrates how the Lambda Hook is deployed. In this model, AWS CloudFormation Lambda Hooks are used to intercept the deployment of Lambda functions and perform compliance checks on those resources.\nThe Lambda Hook interacts with another AWS Lambda function, responsible for performing compliance checks.\nFinally, the system uses AWS Systems Manager Parameter Store to store a configuration parameter containing a list of permitted Lambda runtimes.\nA developer (or CI/CD pipeline) deploys a CloudFormation Stack that includes Lambda functions.\nCloudFormation will call the corresponding Lambda Hook, which is configured to intercept AWS Lambda-related resource operations. In this case, the hook is set up to \u0026ldquo;FAIL\u0026rdquo; the deployment process if the compliance checks fail.\nThe Lambda Hook checks if the runtime of the Lambda function complies with the company’s regulations. To do this, it compares the runtime of the Lambda function against the list of permitted runtimes, which is stored as a Parameter in AWS Systems Manager Parameter Store. Note that in this example, SSM Parameter Store is used to store the configuration, but other services such as Amazon DynamoDB, AWS Secrets Manager, or AppConfig could be used as well.\nAfter checking the runtime’s compliance, the Lambda Hook responds as follows:\nFailure: if the Lambda runtime does not comply with the policy. Success: if the Lambda runtime complies with the company’s policy. Based on the Lambda Hook’s response, the CloudFormation deployment process will either continue or be stopped.\nhook-lambda: a directory containing all source code related to the CloudFormation Lambda Hook, including the Lambda function used for validation (Validation Lambda Function) and the CloudFormation template. sample: a directory containing sample code used to test the CloudFormation Lambda Hook. deploy.sh: a utility script used to deploy the solution via the AWS CLI. cleanup.sh: a utility script used to clean up the infrastructure — CloudFormation Hook on AWS via AWS CLI. template.yml: the CloudFormation template file (AWS CloudFormation Template) containing all the AWS resources used in this solution. Prerequisites: You must meet the following prerequisites for the solution:\nAWS Account — or sign up to create and activate a new AWS account.\nSoftware required on the development machine:\nInstall the AWS Command Line Interface (AWS CLI) and configure it to connect to your AWS account. Install Node.js and use a package manager like npm. Appropriate AWS credentials to interact with the resources in your AWS account. Procedure: Create the AWS Lambda Validation Function – Lambda Code The CloudFormation Lambda Hook will interact with a specific Lambda function (referred to as the Validation Lambda for the rest of the article). This function will be invoked during the CloudFormation CREATE or UPDATE STACK operations involving Lambda functions. The goal is to check whether these Lambda functions are using a runtime that complies with AnyCompany\u0026rsquo;s policies.\nHere’s a detailed description of the steps the Validation Lambda function handler performs (the code is written in TypeScript):\nThe Validation Lambda first retrieves the value of an environment variable — this variable contains the parameter name in AWS Systems Manager Parameter Store where the list of compliant runtimes is stored. Next, the function performs safety checks to ensure that only Lambda resources are considered. It also ensures that the Lambda function\u0026rsquo;s Runtime property is explicitly defined.\nNote: The two safety checks above could theoretically be skipped, as the Hook is already configured to interact only with Lambda resources, and the Lambda Runtime property is always required. However, these checks are kept to demonstrate how to extract information from the Lambda Hook event in your handler.\nconst parameterName = process.env.PERMITTED_RUNTIMES_PARAM; if (!parameterName) { throw new Error(\u0026#39;Permitted Runtimes Parameter is not set\u0026#39;); } const resourceProperties = event.requestData.targetModel.resourceProperties; // Check if this is a Lambda function resource if (event.requestData.targetType !== \u0026#39;AWS::Lambda::Function\u0026#39;) { console.log(\u0026#34;Resource is not a Lambda function, skipping\u0026#34;); return { hookStatus: \u0026#39;SUCCESS\u0026#39;, message: \u0026#39;Not a Lambda function resource, skipping validation\u0026#39;, clientRequestToken: event.clientRequestToken } } // Check runtime version compliance const runtime = resourceProperties.Runtime; if (!runtime) { console.log(\u0026#34;Runtime not defined, failing\u0026#34;); return { hookStatus: \u0026#39;FAILURE\u0026#39;, errorCode: \u0026#39;NonCompliant\u0026#39;, message: \u0026#39;Runtime is required for Lambda functions\u0026#39;, clientRequestToken: event.clientRequestToken } } Next, the Validation Lambda will retrieve the configuration parameter value from the AWS Systems Manager Parameter Store through a utility class called ParameterStoreService. In the example presented in this article, the value inside the configuration parameter is a list of strings, where each string represents a valid AWS Lambda runtime value, such as:\nnodejs22.x, nodejs20.x, python3.11, python3.10, java17, java11, dotnet6\nOnce this value is retrieved, the Validation Lambda will check if the runtime of the Lambda resource being deployed is within the list of permitted runtimes. If the runtime is non-compliant, the function will return a correctly formatted response with hookStatus = FAILURE. If the runtime is compliant, the response will contain hookStatus = SUCCESS.\n// Retrieve configuration from Parameter Store const compliantRuntimes = await parameterStoreService.getParameterFromStore(parameterName); // Check if Lambda runtime is permitted or not if (!compliantRuntimes.includes(runtime)) { console.log(\u0026#34;Runtime \u0026#34; + runtime + \u0026#34; not compliant \u0026#34;); return { hookStatus: \u0026#39;FAILURE\u0026#39;, errorCode: \u0026#39;NonCompliant\u0026#39;, message: `Runtime ${runtime} is not compliant. Please use one of: ${compliantRuntimes.join(\u0026#39;, \u0026#39;)}`, clientRequestToken: event.clientRequestToken } } return { hookStatus: \u0026#39;SUCCESS\u0026#39;, message: \u0026#39;Runtime version compliance check passed\u0026#39;, clientRequestToken: event.clientRequestToken } For more information on possible response values of CloudFormation Lambda Hooks, you can refer to the provided link.\nCreating the Validation Lambda – Lambda Definition in CloudFormation The Validation Lambda will be deployed through CloudFormation, alongside the CloudFormation Lambda Hook definition and the AWS Systems Manager Parameter Store parameter. Below is a fragment of the CloudFormation template describing the configuration for the Validation Lambda:\n# Lambda Function ValidationFunction: Type: AWS::Lambda::Function Properties: Handler: index.handler Role: !GetAtt LambdaExecutionRole.Arn Code: S3Bucket: !Ref DeploymentBucket S3Key: hook-lambda.zip Runtime: nodejs22.x Timeout: 60 MemorySize: 128 Environment: Variables: PERMITTED_RUNTIMES_PARAM: !Ref ParameterStoreParamName You need to assign the Lambda function an IAM (IAM role) with the appropriate permissions to access the parameter in the AWS Systems Manager Parameter Store.\n# Lambda Function Role LambdaExecutionRole: Type: AWS::IAM::Role Properties: AssumeRolePolicyDocument: Version: \u0026#34;2012-10-17\u0026#34; Statement: - Effect: Allow Principal: Service: lambda.amazonaws.com Action: sts:AssumeRole ManagedPolicyArns: - arn:aws:iam::aws:policy/service-role/AWSLambdaBasicExecutionRole # IAM Policy to access Parameter Store ParameterStoreAccessPolicy: Type: AWS::IAM::RolePolicy Properties: RoleName: !Ref LambdaExecutionRole PolicyName: ParameterStoreAccess PolicyDocument: Version: \u0026#34;2012-10-17\u0026#34; Statement: - Effect: Allow Action: - ssm:GetParameter Resource: !Sub arn:aws:ssm:${AWS::Region}:${AWS::AccountId}:parameter${ParameterStoreParamName} Creating the CloudFormation Lambda Hook At this step, you will create a CloudFormation Lambda Hook that meets the following requirements:\nActivated during the CREATE and UPDATE phases of CloudFormation. Only considers resources of type AWS::Lambda::Function. Executes during the “Pre-Provisioning” phase of the CloudFormation template (i.e., before resources are initialized). Applies to both Stack and Resource operations. Calls the previously defined Validation Lambda function. Here’s the corresponding definition in the CloudFormation template:\n# Lambda Hook ValidationHook: Type: AWS::CloudFormation::LambdaHook Properties: Alias: Private::Lambda::LambdaResourcesComplianceValidationHook LambdaFunction: !GetAtt ValidationFunction.Arn ExecutionRole: !GetAtt HookExecutionRole.Arn FailureMode: FAIL HookStatus: ENABLED TargetFilters: Actions: - CREATE - UPDATE InvocationPoints: - PRE_PROVISION TargetNames: - AWS::Lambda::Function TargetOperations: - RESOURCE - STACK Please note that the CloudFormation template above references an IAM role, as the Hook needs the proper permissions to invoke the target Lambda function. Below is the definition for the IAM role:\n# Hook Execution Role HookExecutionRole: Type: AWS::IAM::Role Properties: AssumeRolePolicyDocument: Version: \u0026#34;2012-10-17\u0026#34; Statement: - Effect: Allow Principal: Service: hooks.cloudformation.amazonaws.com Action: sts:AssumeRole # IAM Policy for Lambda Invocation LambdaInvokePolicy: Type: AWS::IAM::RolePolicy Properties: RoleName: !Ref HookExecutionRole PolicyName: LambdaInvokePolicy PolicyDocument: Version: \u0026#34;2012-10-17\u0026#34; Statement: - Effect: Allow Action: - lambda:InvokeFunction Resource: !GetAtt ValidationFunction.Arn Configuring Compliant Runtimes - Using Systems Manager Parameter Store AWS Systems Manager Parameter Store is a secure, hierarchical service for managing configuration data and secrets, allowing users to store and retrieve data such as configuration, database strings, etc., in the form of parameter values.\nIn this specific example, we will utilize Parameter Store to store the list of allowed Lambda runtimes. This configuration value will be a StringList parameter containing a list of permitted runtimes, separated by commas. Below is a sample CloudFormation snippet defining the parameter:\n# Parameter Store Parameter ConfigParameter: Type: AWS::SSM::Parameter Properties: Name: !Ref ParameterStoreParamName Type: StringList Value: !Ref ParameterStoreDefaultValue Description: \u0026#34;Configuration for Lambda Hook\u0026#34; Note: The use of parameters in CloudFormation for the Name and Value attributes enables dynamic input when deploying the CloudFormation template.\nDeploying the Solution To deploy this solution, you can use the deploy.sh script located in the root of the repository.\nThis script will perform the following steps:\nCompile and build the Validation Lambda function. Create an Amazon S3 bucket to store the CloudFormation template. Upload the CloudFormation template and Lambda source code to the S3 bucket. Deploy the CloudFormation template. Testing the Lambda Hook To test the CloudFormation Lambda Hook, you can deploy a simple CloudFormation template containing a \u0026ldquo;Hello World\u0026rdquo; Lambda function. First, test the Lambda function configured with a compliant runtime. Then, modify the template to configure the Lambda function with a non-compliant runtime.\nBelow is the initial definition of the CloudFormation template for testing:\n# Lambda Function HelloWorldFunction: Type: AWS::Lambda::Function Properties: FunctionName: hello-world-function Runtime: nodejs22.x Handler: index.handler Role: !GetAtt LambdaExecutionRole.Arn Code: ZipFile: | exports.handler = async (event, context) =\u0026gt; { console.log(\u0026#39;Hello World!\u0026#39;); const response = { statusCode: 200, body: JSON.stringify(\u0026#39;Hello World!\u0026#39;) }; return response; }; Timeout: 30 Note that the runtime value is nodejs22.x, which is currently within the list of allowed runtimes. Therefore, the expectation is that the deployment of this Lambda function will succeed.\nDeploy this template using the AWS CLI:\naws cloudformation deploy \\ --template-file sample/template.yml \\ --stack-name hook-test \\ --capabilities CAPABILITY_IAM Check the CloudFormation Console:\nAs expected, the deployment will succeed. You can also verify that the CloudFormation Lambda Hook was triggered by checking CloudWatch Logs.\nNow, edit the original template to set up a Lambda runtime not included in the list of permitted runtimes:\n# Lambda Function HelloWorldFunction: Type: AWS::Lambda::Function Properties: FunctionName: hello-world-function Runtime: nodejs18.x Handler: index.handler Role: !GetAtt LambdaExecutionRole.Arn Code: ZipFile: | exports.handler = async (event, context) =\u0026gt; { console.log(\u0026#39;Hello World!\u0026#39;); const response = { statusCode: 200, body: JSON.stringify(\u0026#39;Hello World!\u0026#39;) }; return response; }; Timeout: 30 MemorySize: 128 Deploy this template using the AWS CLI with the same command and check the CloudFormation Console:\nAs expected, the deployment will fail. The CloudFormation Lambda Hook was triggered, and since the Lambda runtime is not in the list of allowed runtimes, the deployment was blocked. You can also verify that the Hook failed in CloudWatch Logs.\nCleaning Up Resources: To remove the resources related to the test template, you can run the cleanup_sample.sh script in the sample directory. This script will delete\nTo delete the resources related to the main solution described above (based on the AWS CloudFormation Lambda Hook), you can use the cleanup.sh script in the root folder of the repository. This script will perform the following tasks:\nDelete the CloudFormation Stack Empty the S3 Bucket used for the deployment of the Stack Delete the S3 Bucket Conclusion: In this article, you learned how to implement CloudFormation Hooks to ensure compliance with Lambda runtime across your entire AWS infrastructure. By leveraging the power of Lambda Hooks, you learned how to create a preventive control mechanism to validate Lambda runtime configurations before deployment.\nBy enabling Lambda Hooks and deploying a custom Lambda function for validation, you have established an automated process that ensures only compliant runtimes are used in your organization\u0026rsquo;s Lambda functions when creating or updating CloudFormation Stacks. This solution can be easily integrated with popular development tools like AWS CLI, AWS SAM, CI/CD pipelines, and AWS CDK, helping streamline control implementation in your current workflows and eliminating the need for manual checks or post-deployment fixes.\nThe validation method presented in this article is not limited to Lambda runtimes but can be extended to other AWS resources supported by CloudFormation, enabling you to enforce governance policies across various infrastructure components in your AWS environment.\nAbout the Authors: Matteo Luigi Restelli Matteo Luigi Restelli is a Senior Partner Solutions Architect at AWS. He primarily works with AWS consulting partners in Italy and specializes in areas such as Infrastructure as Code, Cloud Native App Development, and DevOps. Outside of work, he enjoys swimming, rock \u0026amp; roll music, and learning new things every day, particularly in the field of Computer Science.\nStella Hie Stella Hie is a Senior Product Manager Technical responsible for AWS Infrastructure as Code. She focuses on proactive control and governance, with the goal of providing customers with the best experience when using AWS solutions securely. Outside of work, she enjoys mountain climbing, playing the piano, and attending live shows.\n"},{"uri":"https://hd-2004.github.io/AWS_Fall2025_Workshop/","title":"Internship Report","tags":[],"description":"","content":"Internship Report Student Information: Full Name: Đỗ Hoàng Hiếu\nPhone Number: 0773780253\nEmail: hieudhse184340@fpt.edu.vn\nUniversity: FPT University\nMajor: Information Assurance\nClass: AWS082025\nInternship Company: Amazon Web Services Vietnam Co., Ltd.\nInternship Position: FCJ Cloud Intern\nInternship Duration: From 08/09/2025 to 04/12/2025\nReport Content Worklog Proposal Translated Blogs Events Participated Workshop Self-evaluation Sharing and Feedback "},{"uri":"https://hd-2004.github.io/AWS_Fall2025_Workshop/5-workshop/5.1-workshop-overview/","title":"Introduction","tags":[],"description":"","content":"Project Introduction This workshop presents the architecture and implementation of English Journey,\na vocabulary-learning web application built on AWS.\nThe application allows students to:\nsign up and sign in securely, take a level placement test (A1–C1) before learning, practise vocabulary and quizzes, track their own learning progress. On the AWS side, the project demonstrates how to combine several managed services:\nAWS Amplify as the central platform for the web app backend and hosting, Amazon Cognito for authentication, AWS Lambda for backend logic (Level test, Quiz, Vocabulary), Amazon DynamoDB for application data, Amazon SES for sending email notifications and alerts to learners (Account Verification), Amazon CloudWatch for logs, metrics, AWS WAF for basic web application protection, and IAM Roles \u0026amp; Policies to control access between all components. Workshop Objectives By the end of this workshop, a reader should be able to:\nUnderstand the overall architecture of the English Journey web app on AWS. Explain the role of Amplify and how it orchestrates Cognito, Lambda, DynamoDB and S3. Describe how the level-test feature connects frontend, Lambda and DynamoDB. Understand how notifications and system alerts are delivered via email using Amazon SES. Recognise the importance of CloudWatch and IAM for monitoring and security. Workshop Overview This project leverages AWS services to build and deploy the application:\nAWS Amplify: A full-stack hosting service that enables quick and easy deployment of applications. AWS Lambda: Handles application tasks and logic without the need to manage servers, saving costs and resources. Amazon DynamoDB: A NoSQL database used to store user data, vocabulary, and learning results. Amazon S3: Stores learning materials (videos, audio, images) to support the learning process. Amazon CloudWatch: Monitors the performance and operation of the application, providing logs and alerts in case of issues. !\n"},{"uri":"https://hd-2004.github.io/AWS_Fall2025_Workshop/4-eventparticipated/4.1-event1/","title":"Event 1","tags":[],"description":"","content":"Summary Report: “Kick-off AWS FCJ Workforce - FPTU OJT FALL 2025” Event About the AWS First Cloud Journey Workforce Program Launched in 2021, the program has accompanied more than 2,000 students across the country.\nOver 150 learners have received intensive training and are now working at leading technology companies in Vietnam and internationally.\nMain Objectives: Build a high-quality generation of AWS Builders for Vietnam.\nEquip students with practical skills in Cloud, DevOps, AI/ML, Security, Data \u0026amp; Analytics.\nConnect students with the AWS Study Group community of 47,000+ members and AWS partner enterprises.\nThe program is not only about technology training but also serves as a vital bridge between knowledge – technology – career, helping students confidently integrate into the modern tech world and global community.\nEven Name: Event Name: Kick-off AWS FCJ Workforce – FPTU OJT FALL 2025 Time: 08:30, September 6, 2025 Venue: 26th Floor, Bitexco Tower, 02 Hai Trieu Street, Saigon Ward, Ho Chi Minh City Role in Event:: Participant\nSpeakers and Guests School Representative: Mr. Nguyễn Trần Phước Bảo – Head of Corporate Relations Department (QHDN), Opening Speech Accompanied by 2–3 members from the Corporate Relations Department Keynote \u0026amp; Industry Sharing AWS First Cloud Journey \u0026amp; Future Orientation 👤 Nguyễn Gia Hưng – Head of Solutions Architect, AWS Vietnam DevOps \u0026amp; Future Career 👤 Đỗ Huy Thắng – DevOps Lead, VNG Alumni \u0026amp; Career Sharing From First Cloud Journey to GenAI Engineer 👤 Danh Hoàng Hiếu Nghị – GenAI Engineer, Renova She in Tech \u0026amp; Journey with First Cloud Journey 👤 Bùi Hồ Linh Nhi – AI Engineer, SoftwareOne A Day as a Cloud Engineer 👤 Phạm Nguyễn Hải Anh – Cloud Engineer, G-Asia Pacific Journey to First Cloud Journey 👤 Nguyễn Đồng Thanh Hiệp – Principal Cloud Engineer, G-Asia Pacific ✨ Overal Today’s Kick-off event marks the beginning of the AWS Builders journey – where students not only gain access to the most advanced cloud technologies but also receive inspiration, connect with experts, and expand career opportunities.\n"},{"uri":"https://hd-2004.github.io/AWS_Fall2025_Workshop/2-proposal/","title":"Proposal","tags":[],"description":"","content":"This section summarizes the contents of the workshop you plan to conduct.\nStudying English Website 1. Executive Summary The Studying English Website is designed for learners aiming to improve their vocabulary, grammar, and daily communication skills. The platform leverages AWS Serverless services to provide learning time monitoring, predictive analysis of learners’ abilities to offer learning policies from basic to advanced levels, while minimizing costs.\n2. Problem Statement Current Problem\nEnglish is an essential language for work and daily life. However, learners currently lack space and an environment for practice, especially for communication.\nSolution\nTo address the lack of an English practice environment and support learners in improving vocabulary, grammar, and communication skills, we propose building the Studying English Website on the AWS serverless platform. It enables personalized learning based on user data, integrates listening-speaking exercises and instructional videos with learner recordings stored on S3, tracks and analyzes learning progress via AWS Lambda, ensures security and user management through Cognito, IAM, and rapidly deploys a cost-effective web interface using AWS Amplify, providing a flexible, safe, and effective learning environment while helping administrators improve teaching methods based on real data.\nBenefits and Return on Investment (ROI)\nThe Studying English Website helps learners enhance their English skills in a personalized and flexible way, reducing time and cost compared to traditional learning methods. It also provides learning progress analytics to administrators for optimized teaching methods. With low AWS infrastructure costs (~6.45 USD/month), the project has a fast ROI potential through increased learning efficiency and expanded user base, while creating valuable data for AI projects and long-term analytics.\n3. Solution Architecture The architecture of the Studying English Website is based on the AWS serverless platform, using S3 to store raw and processed data, Amplify Gen 2 for web interface deployment Route53 for DNS and routing management, Cognito for user authentication and management, Secrets Manager for sensitive information security, IAM for access control, Lambda for event-driven serverless logic, and WAF to protect the application from attacks, creating a flexible, personalized, secure, and scalable English learning system.\nAWS Services Used\nAWS Amplify Gen 2: Host the web interface AWS Route53: Manage DNS and routing AWS Cognito: Authenticate and manage users AWS IAM: Manage AWS access permissions AWS Lambda: Run serverless code triggered by events AWS WAF: Protect the web application from attacks Component Design\nData Ingestion: Data from users and sources is sent to AWS Lambda, which triggers processing workflows. Data Storage: Raw and processed data are stored in many separate S3 buckets, forming a data lake and ready-to-analyze data repository. Data Processing: AWS Lambda handles serverless events, MediaConvert converts video/audio, and data is indexed. Web Interface: AWS Amplify Gen 2 hosts a Next.js application providing dashboards, real-time analytics, and user data access. User Management: Amazon Cognito handles authentication and user access management, combined with AWS IAM for service access control, securing sensitive information via AWS Secrets Manager, and protecting the entire application with AWS WAF. DNS and routing are managed by Route53. 4. Technical Deployment Deployment Phases\nThe project consists of two parts — building the Studying English Website — each with four phases:\nResearch and Architecture Design: Study and design AWS Serverless architecture (1 month before internship). Cost Calculation and Feasibility Check: Use AWS Pricing Calculator to estimate infrastructure costs and adjust services to ensure feasibility and cost-efficiency (Month 1). Architecture Adjustment for Cost/Performance Optimization: Refine services (e.g., optimize Lambda, MediaConvert, Amplify) and data workflows for maximum efficiency (Month 2). Development, Testing, Deployment: Deploy AWS services using CDK/SDK, develop Next.js interface on Amplify, test the system, and put it into operation (Month 2–3). Technical Requirements\nThe system requires a stable internet connection to operate AWS services, including storing and retrieving data on S3, serverless data processing via Lambda, deploying Next.js web interface on Amplify Gen 2, DNS and routing management via Route53, user authentication and access control with Cognito, sensitive information security via Secrets Manager, service access control via IAM, WAF application protection, as well as supporting data analytics and real-time dashboards.\n5. Roadmap \u0026amp; Milestones Before Internship (Month 0): Study plan preparation Internship (Month 1–3): Month 1: Learn AWS and upgrade hardware Month 2: Learn deployment, plan and design architecture Month 3: Deploy, test, and launch Post Deployment: Research potential development and new features 6. Budget Estimation See costs on AWS Pricing Calculator\nOr download budget estimation file.\nInfrastructure Costs\nAWS Amplify Gen 2: 0.35 USD/month (256 MB, 500 ms request) AWS Route53: 0.50 USD/month (1 domain, 1 million queries) AWS Cognito: 0.00 USD/month (5 Free Tier users) AWS IAM: 0 USD/month AWS Lambda: 0.00 USD/month (1,000 requests, 512 MB RAM) AWS WAF: 5.00 USD/month (1 basic Web ACL) Total: 5.85 USD/month, ~70.2 USD/12 months\n7. Risk Assessment Risk Matrix\nServer downtime: High impact, medium probability Budget overrun: Medium impact, high probability Mitigation Strategy\nCosts: Use AWS Budget for alerts, optimize services Contingency Plan\nRevert to manual data collection if AWS services fail. 8. Expected Outcomes Technical Improvement: Real-time data and analytics replace manual processes. Scalable to 10–15 stations.\nLong-term Value: One-year data platform for AI research, reusable for future projects.\n"},{"uri":"https://hd-2004.github.io/AWS_Fall2025_Workshop/1-worklog/1.10-week10/","title":"Week 10 Worklog","tags":[],"description":"","content":"Week 10 Objectives: Connect and get acquainted with members of First Cloud Journey. Understand basic AWS services, how to use the console \u0026amp; CLI. Tasks to be carried out this week: Day Task Start Date Completion Date Reference Material 2 - Research and work on the group project 11/03/2025 11/07/2025 3 - Research and work on the group project 11/03/2025 11/07/2025 4 - Research and work on the group project 11/03/2025 11/07/2025 5 - Research and work on the group project 11/03/2025 11/07/2025 6 - Research and work on the group project 11/03/2025 11/07/2025 Week 10 Achievements: Making good progress in researching and working on the group project. "},{"uri":"https://hd-2004.github.io/AWS_Fall2025_Workshop/1-worklog/1.11-week11/","title":"Week 11 Worklog","tags":[],"description":"","content":"Week 11 Objectives: Connect and get acquainted with members of First Cloud Journey. Understand basic AWS services, how to use the console \u0026amp; CLI. Tasks to be carried out this week: Day Task Start Date Completion Date Reference Material 2 - Research and work on the group project 11/10/2025 11/14/2025 3 - Research and work on the group project 11/10/2025 11/14/2025 4 - Research and work on the group project 11/10/2025 11/14/2025 5 - Research and work on the group project 11/10/2025 11/14/2025 6 - Research and work on the group project 11/10/2025 11/14/2025 Week 10 Achievements: Making good progress in researching and working on the group project. "},{"uri":"https://hd-2004.github.io/AWS_Fall2025_Workshop/1-worklog/1.12-week12/","title":"Week 12 Worklog","tags":[],"description":"","content":"Week 12 Objectives: Connect and get acquainted with members of First Cloud Journey. Understand basic AWS services, how to use the console \u0026amp; CLI. Tasks to be carried out this week: Day Task Start Date Completion Date Reference Material 2 - Attend AWS Cloud Mastery Series #2 event 11/17/2025 11/17/2025 3 - Continue researching and working on the group project 11/18/2025 11/21/2025 4 - Continue researching and working on the group project 11/18/2025 11/21/2025 5 - Continue researching and working on the group project 11/18/2025 11/21/2025 6 - Continue researching and working on the group project 11/18/2025 11/21/2025 Week 12 Achievements: *Making good progress in researching and working on the group project.\n"},{"uri":"https://hd-2004.github.io/AWS_Fall2025_Workshop/5-workshop/5.2-prerequiste/","title":"Prerequiste","tags":[],"description":"","content":"IAM permissions Add the following IAM permission policy to your user account to deploy and cleanup this workshop.\n{ \u0026#34;Version\u0026#34;: \u0026#34;2012-10-17\u0026#34;, \u0026#34;Statement\u0026#34;: [ { \u0026#34;Sid\u0026#34;: \u0026#34;CognitoPermissions\u0026#34;, \u0026#34;Effect\u0026#34;: \u0026#34;Allow\u0026#34;, \u0026#34;Action\u0026#34;: [ \u0026#34;cognito-idp:AdminCreateUser\u0026#34;, \u0026#34;cognito-idp:AdminUpdateUserAttributes\u0026#34;, \u0026#34;cognito-idp:ListUsers\u0026#34;, \u0026#34;cognito-idp:SignUp\u0026#34;, \u0026#34;cognito-idp:InitiateAuth\u0026#34; ], \u0026#34;Resource\u0026#34;: \u0026#34;*\u0026#34; }, { \u0026#34;Sid\u0026#34;: \u0026#34;LambdaPermissions\u0026#34;, \u0026#34;Effect\u0026#34;: \u0026#34;Allow\u0026#34;, \u0026#34;Action\u0026#34;: [ \u0026#34;lambda:InvokeFunction\u0026#34;, \u0026#34;lambda:ListFunctions\u0026#34;, \u0026#34;lambda:CreateFunction\u0026#34;, \u0026#34;lambda:UpdateFunctionCode\u0026#34; ], \u0026#34;Resource\u0026#34;: \u0026#34;*\u0026#34; }, { \u0026#34;Sid\u0026#34;: \u0026#34;DynamoDBPermissions\u0026#34;, \u0026#34;Effect\u0026#34;: \u0026#34;Allow\u0026#34;, \u0026#34;Action\u0026#34;: [ \u0026#34;dynamodb:PutItem\u0026#34;, \u0026#34;dynamodb:GetItem\u0026#34;, \u0026#34;dynamodb:UpdateItem\u0026#34;, \u0026#34;dynamodb:Query\u0026#34;, \u0026#34;dynamodb:Scan\u0026#34; ], \u0026#34;Resource\u0026#34;: \u0026#34;arn:aws:dynamodb:region:account-id:table/your-table-name\u0026#34; }, { \u0026#34;Sid\u0026#34;: \u0026#34;SESPermissions\u0026#34;, \u0026#34;Effect\u0026#34;: \u0026#34;Allow\u0026#34;, \u0026#34;Action\u0026#34;: [ \u0026#34;ses:SendEmail\u0026#34;, \u0026#34;ses:SendRawEmail\u0026#34; ], \u0026#34;Resource\u0026#34;: \u0026#34;*\u0026#34; } { \u0026#34;Sid\u0026#34;: \u0026#34;CloudWatchPermissions\u0026#34;, \u0026#34;Effect\u0026#34;: \u0026#34;Allow\u0026#34;, \u0026#34;Action\u0026#34;: [ \u0026#34;cloudwatch:PutMetricData\u0026#34;, \u0026#34;cloudwatch:GetMetricData\u0026#34;, \u0026#34;cloudwatch:DescribeAlarms\u0026#34;, \u0026#34;cloudwatch:SetAlarmState\u0026#34; ], \u0026#34;Resource\u0026#34;: \u0026#34;*\u0026#34; }, { \u0026#34;Sid\u0026#34;: \u0026#34;WAFPermissions\u0026#34;, \u0026#34;Effect\u0026#34;: \u0026#34;Allow\u0026#34;, \u0026#34;Action\u0026#34;: [ \u0026#34;wafv2:CreateWebACL\u0026#34;, \u0026#34;wafv2:UpdateWebACL\u0026#34;, \u0026#34;wafv2:GetWebACL\u0026#34;, \u0026#34;wafv2:ListWebACLs\u0026#34; ], \u0026#34;Resource\u0026#34;: \u0026#34;*\u0026#34; }, { \u0026#34;Sid\u0026#34;: \u0026#34;IAMPermissions\u0026#34;, \u0026#34;Effect\u0026#34;: \u0026#34;Allow\u0026#34;, \u0026#34;Action\u0026#34;: [ \u0026#34;iam:CreateRole\u0026#34;, \u0026#34;iam:AttachRolePolicy\u0026#34;, \u0026#34;iam:ListPolicies\u0026#34;, \u0026#34;iam:GetRole\u0026#34; ], \u0026#34;Resource\u0026#34;: \u0026#34;*\u0026#34; } ] } Technical background This workshop assumes that the reader has basic knowledge of:\nWeb development\nHTML, CSS and JavaScript/TypeScript React or another single-page application framework Cloud and AWS fundamentals\nwhat an AWS region and account are, the idea of managed services (Cognito, Lambda, DynamoDB,…), basic concepts of IAM (identity, role, policy, least privilege). The report is written so that it can be understood even if the reader does not actually deploy the system, but this background helps to follow the architecture.\nTools and services To reproduce the workshop in a real environment, the following tools and services would be required:\nAn AWS account with permission to create:\nAmplify apps, Cognito User Pools, Lambda functions, DynamoDB tables, SES identities and configuration sets, IAM roles and policies. Node.js and npm installed locally\n(for running and building the React frontend).\nThe AWS CLI configured with an IAM user or role that has sufficient permissions.\nOptionally, the Amplify CLI / Gen 2 tooling\nto define infrastructure in code and connect the project to Amplify.\nSource code and project structure The English Journey project is organised as:\na React frontend (pages such as Level Test, Dictionary, Vocabulary, My Learning), a backend defined via Amplify (Cognito, Lambda, DynamoDB), additional infrastructure for SES (email), CloudWatch and WAF. In this Hugo workshop site, we only present the architecture diagrams, explanations and example code. The actual AWS resources do not need to be created to understand the design decisions.\nThe following sections (5.3 and onwards) build on these prerequisites and explain each group of AWS services in more detail.\n"},{"uri":"https://hd-2004.github.io/AWS_Fall2025_Workshop/5-workshop/5.3-create-amplify/","title":"Create Amplify backend","tags":[],"description":"","content":"Goal In this step we describe how the Amplify backend for the English Journey web application was created.\nInstead of provisioning Amazon Cognito, AWS Lambda, Amazon S3, AWS WAF and Amazon DynamoDB separately in the console, we use AWS Amplify (Gen 2) as a central orchestration layer. Amplify reads the configuration from our project and generates the necessary AWS resources for authentication, APIs, data storage and hosting.\n5.3.1 Why Amplify? English Journey is a single-page application built with React. We chose Amplify because:\nit provides one entry point to configure the backend for a web or mobile app, it can automatically create Cognito, Lambda, DynamoDB, S3 and CloudFront resources from a simple configuration, it integrates easily with the Amplify JavaScript libraries used by the frontend (sign-in, API calls, file storage). Amplify becomes the “backend platform” that hides much of the low-level boilerplate of setting up these services one by one.\n5.3.2 Amplify app creation and hosting We created a new Amplify App from the AWS Management Console and connected it to our Git repository that contains the React source code of English Journey. During the wizard we configured: the default build settings (install dependencies and run npm run build), the environment variables required by the app. Amplify automatically created: an S3 bucket to store the built static files, a CloudFront distribution in front of that bucket to serve the web site with low latency. The output of this step is a public URL where the React frontend of English Journey is hosted. All later backend resources (Cognito, Lambda, DynamoDB…) are associated with this Amplify app. 5.3.3 Authentication with Cognito (Amplify Auth) To implement sign-up, sign-in and password reset for students, we enabled the Auth category in Amplify.\nConceptually, the steps were:\nDefine the authentication requirement in Amplify: sign-in with email and password, self-registration enabled so that new students can create accounts, basic password policy and email verification. Amplify generated an Amazon Cognito User Pool with the corresponding settings. The React frontend uses the Amplify Auth library to: create new users (sign up), authenticate existing users (sign in), read user attributes (name, email) and display greetings such as “Chào mừng trở lại, Duy Khang!”. All tokens issued by Cognito (ID token and access token) are later used to protect our API and Lambda functions.\n5.3.4 Backend logic with Lambda (Amplify Functions) The main business logic of English Journey runs in AWS Lambda.\nUsing Amplify’s Function category we created several Lambda functions, for example:\nMyLearning / DailyCheckIn – updates study streaks and progress for the user. LevelTest – receives the answers from the placement test, calculates the CEFR level (A1–C2) and stores the result. Dictionary / Vocabulary – provides APIs for searching words, saving “bookmarked” vocabulary and tracking which words a user has mastered. From the Amplify project these functions are defined as backend handlers.\nWhen deploying the Amplify app, each function is created as a separate Lambda with the correct IAM permissions and environment variables (for example, DynamoDB table names).\n5.3.5 Data layer with DynamoDB To store application data we defined several data models in the backend, which Amplify maps to Amazon DynamoDB tables:\nUsers / Profiles – basic user information and learning preferences. PlacementTestResults – scores and detected level for each attempt. Vocabulary / Dictionary – list of words, meanings, examples and CEFR levels. UserProgress – saved vocabulary, words marked as “mastered”, quiz history, daily streaks. Each Lambda function receives the table name via environment variables generated by Amplify, and accesses DynamoDB through the AWS SDK.\nUsing Amplify in this way keeps all table definitions in code and makes the deployment reproducible.\n5.3.7 Protection with AWS WAF The public endpoint of the web application is the CloudFront distribution created by Amplify.\nTo protect this endpoint we associated an AWS WAF Web ACL with the distribution and enabled:\nthe AWS managed rule groups that block common web attacks (SQL injection, XSS, bot traffic), a basic rate-limit rule to prevent simple denial-of-service attempts. In the architecture diagram this is represented by the AWS WAF component in front of the Amplify application.\nSummary In summary, Amplify is the central service that creates and connects:\nCognito for authentication, Lambda for backend logic, DynamoDB for application data, and integrates with AWS WAF for additional protection. The rest of the workshop (SNS, CloudWatch, IAM policies, …) builds on top of this Amplify-managed backend.\n"},{"uri":"https://hd-2004.github.io/AWS_Fall2025_Workshop/3-blogstranslated/","title":"Translated Blogs","tags":[],"description":"","content":"This section will list and introduce the blogs you have translated. For example:\nBlog 1 - Visually build telephony applications with AWS Step Functions This blog demonstrates how combining AWS Step Functions and the Amazon Chime SDK’s PSTN audio service simplifies the development of reliable telephony applications through visual workflow design and error management. We provided a sample application, implementing six core features for business telephony, illustrating how this solution effectively manages multiple conditional paths and exceptions such as disconnects and invalid inputs. The serverless architecture created enables seamless integration between the two services via JSON-based communication, providing automatic scaling\nBlog 2 - Visualize and gain insights into your VPC with Amazon Q in Amazon QuickSight In this article, you learned how to use Amazon QuickSight to visualize data from multiple data sources using natural language queries. Amazon Q in QuickSight helps democratize data access, empowering everyone in your organization to make data-driven decisions. Organizing data into visual topics and enabling natural language querying allows Amazon Q users to better understand VPC flows. To help your team get started with QuickSight, we recommend reviewing the following guides: What is Amazon QuickSight Q and Best Practices for QuickSight Q Authors.\nBlog 3 - Validate Your Lambda Runtime with CloudFormation Lambda Hooks In this article, you learned how to implement CloudFormation Hooks to ensure compliance with Lambda runtime across your entire AWS infrastructure. By leveraging the power of Lambda Hooks, you learned how to create a preventive control mechanism to validate Lambda runtime configurations before deployment.\nBy enabling Lambda Hooks and deploying a custom Lambda function for validation, you have established an automated process that ensures only compliant runtimes are used in your organization\u0026rsquo;s Lambda functions when creating or updating CloudFormation Stacks. This solution can be easily integrated with popular development tools like AWS CLI, AWS SAM, CI/CD pipelines, and AWS CDK, helping streamline control implementation in your current workflows and eliminating the need for manual checks or post-deployment fixes.\nThe validation method presented in this article is not limited to Lambda runtimes but can be extended to other AWS resources supported by CloudFormation, enabling you to enforce governance policies across various infrastructure components in your AWS environment.\n"},{"uri":"https://hd-2004.github.io/AWS_Fall2025_Workshop/5-workshop/5.4-aws-cognito/","title":"Configure AWS Cognito","tags":[],"description":"","content":"Goal In this step, we describe how Amazon Cognito is used in the English Journey web application to manage user authentication.\nStudents can sign in using:\nEmail \u0026amp; password Google (Gmail) account Cognito acts as the central identity provider that issues tokens for the frontend and backend of English Journey.\n5.4.1 Role of Amazon Cognito in the architecture In the architecture of English Journey:\nCognito User Pool stores user identities (email, name, etc.). It handles sign-up, sign-in, password reset and email verification. It integrates with Amplify so the React frontend can easily call signIn, signUp and other auth functions. It also connects to social identity providers: Google → for users who want to log in with their Gmail All successful logins (email, Google) result in Cognito tokens that are used to protect our APIs and Lambda functions.\n5.4.2 Creating the Cognito User Pool The main configuration steps:\nCreate a User Pool\nSign in to the AWS Management Console → Amazon Cognito → User pools → Create user pool. Choose Email as the primary sign-in identifier. Allow self-registration so students can create their own accounts. Configure password policy \u0026amp; verification\nSet a basic password policy (minimum length, characters, etc.). Enable email verification so students must confirm their email before using the app. Customize email templates (optional) for sign-up and password reset. Create an app client\nCreate a public app client for the web frontend. Enable Cognito User Pool and social IdPs as allowed identity providers. Configure allowed callback URLs and sign-out URLs (for example, the Amplify frontend domain of English Journey). This user pool is later referenced by Amplify and the React frontend.\n5.4.3 Enabling Google (Gmail) To support social logins, we added Google as identity providers in Cognito.\nGoogle (Gmail) login In Google Cloud Console, create an OAuth 2.0 Client ID for a web application. Set the authorized redirect URI to the Cognito callback URL generated for the user pool. Copy the Client ID and Client Secret from Google. In Cognito → User pool → Identity providers → Google: Paste the Client ID and Client Secret. Map Google attributes (email, name) to Cognito standard attributes. Now users can click \u0026ldquo;Sign in with Google\u0026rdquo; and authenticate using their Gmail account.\n5.4.4 Integrating Cognito with the Amplify frontend The React frontend of English Journey uses AWS Amplify Auth to communicate with Cognito.\nConceptually:\nFor email/password: Auth.signUp() is used to create a new user. Auth.signIn() is used for normal login. For Gmail social login: We call Auth.federatedSignIn({ provider: 'Google' }), Amplify redirects the user to Google, then back to Cognito, then back to the web app with valid tokens. On the UI, the login page shows three main options:\nSign in with email \u0026amp; password Sign in with Google All three methods still end up in the same Cognito User Pool.\n5.4.5 Security and user management With Cognito, we can:\nRequire email verification before granting full access to the application.\nRestrict which domains are allowed to use the sign-in flow (via callback URLs).\nManage users centrally:\nLock accounts Reset passwords Delete accounts Extend the solution later with:\nMFA (Multi-Factor Authentication) Additional social providers (GitHub, Facebook, …) Within the scope of this workshop, we focus on:\nSign-in with email \u0026amp; password Application login via Gmail Email verification (OTP) Summary In this step, we configure Amazon Cognito as the identity management service for English Journey:\nCreate a User Pool to manage accounts and authenticate users. Allow users to sign in with email and Google (Gmail). Use tokens issued by Cognito in the React + Amplify front end to securely access APIs and back-end services. "},{"uri":"https://hd-2004.github.io/AWS_Fall2025_Workshop/4-eventparticipated/","title":"Events Participated","tags":[],"description":"","content":" In this section, you should list and describe in detail the events you have participated in during your internship or work experience.\nEach event should be presented in the format Event 1, Event 2, Event 3…, along with the following details:\nEvent name Date and time Location (if applicable) Your role in the event (attendee, event support, speaker, etc.) A brief description of the event’s content and main activities Outcomes or value gained (lessons learned, new skills, contribution to the team/project) This listing helps demonstrate your actual participation as well as the soft skills and experience you have gained from each event. During my internship, I participated in two events. Each one was a memorable experience that provided new, interesting, and useful knowledge, along with gifts and wonderful moments.\nEvent 1 Event name: Kick-off AWS FCJ Workforce - FPTU OJT FALL 2025\nTime: 08:30 on 06/09/2025\nLocation: 26th floor, Bitexco Tower, 02 Hai Trieu Street, Saigon Ward, Ho Chi Minh City\nRole in the event: Attendee\nEvent 2 Event name: AI-Driven Development Life Cycle: Reimagining Software Engineering\nTime: 14:00 on 03/10/2025\nLocation: 26th floor, Bitexco Tower, 02 Hai Trieu Street, Saigon Ward, Ho Chi Minh City\nRole in the event: Attendee\nEvent 3 Event name: AI/ML/GenAI on AWS\nTime: 8:30 on 15/11/2025\nLocation: 26th floor, Bitexco Tower, 02 Hai Trieu Street, Saigon Ward, Ho Chi Minh City\nRole in the event: Attendee\nEvent 4 Event name: DevOps on AWS\nTime: 8:30 on 17/11/2025\nLocation: 26th floor, Bitexco Tower, 02 Hai Trieu Street, Saigon Ward, Ho Chi Minh City\nRole in the event: Attendee\nEvent 5 Event name: According to AWS Well-Architected Security Pillar\nTime: 08:30 on 29/11/2025\nLocation: 26th floor, Bitexco Tower, 02 Hai Trieu Street, Saigon Ward, Ho Chi Minh City\nRole in the event: Attendee\n"},{"uri":"https://hd-2004.github.io/AWS_Fall2025_Workshop/5-workshop/5.5-create-ses/","title":"Configure Amazon SES for email","tags":[],"description":"","content":"Goal SES will be used to send transactional emails to learners\nIt can later be extended to:\nSend welcome emails when an account is created. Send study reminder emails. Note: SES is a regional service. Make sure you are working in the same AWS Region that you used for Amplify, Lambda and DynamoDB.\n5.5.1 – Open the Amazon SES console From the AWS Management Console, type “SES” in the search box. Select Amazon Simple Email Service. Check the Region in the top-right corner (for example: ap-southeast-1). If SES is in a different Region, switch to the Region used for this workshop environment. 5.5.2 – Verify an email identity For this workshop we will verify a single email address and send emails from that address (for example: your personal Gmail).\nIn the SES left navigation menu, choose Verified identities. Click Create identity. Select Email address. In Email address, enter the address you will use as the sender, for example:\nyour-name+english-journey@gmail.com. Leave the other options as default and choose Create identity. SES sends a verification email to that address. Open your inbox, find the email from Amazon Web Services, and click the verification link. Return to the SES console and refresh. The identity status should become Verified. From now on, SES will only allow sending emails from and to verified identities (SES Sandbox mode). This is enough for a classroom / workshop environment.\n5.5.3 – (Optional) Move out of Sandbox mode If later you want to send emails to real learners (unverified addresses), you must move your SES account out of sandbox mode:\nIn the SES console, go to Account dashboard. Under Your account details, check the Account status. If it is still Sandbox, click Request production access and follow the instructions. Within the scope of this workshop, you can stay in sandbox mode as long as you only send emails between verified addresses.\n5.5.4 – Create a configuration set (optional but recommended) A configuration set groups all emails from the English Journey application and enables future observability (CloudWatch metrics, event publishing, etc.).\nIn the SES navigation menu, choose Configuration sets. Click Create configuration set. Enter a name, for example: english-journey-config. Leave all other settings as default and click Create configuration set. We will use this configuration set later when sending emails from Lambda functions.\n5.5.5 – Allow Lambda to send email with SES Lambda functions such as Daily Check-in or Test Level result will send emails through SES.\nLambda needs permission to call the SES APIs.\nYou will attach this permission in Section 5.7 – Create IAM Roles \u0026amp; Policies, but we prepare the policy here for reference:\n{ \u0026#34;Version\u0026#34;: \u0026#34;2012-10-17\u0026#34;, \u0026#34;Statement\u0026#34;: [ { \u0026#34;Effect\u0026#34;: \u0026#34;Allow\u0026#34;, \u0026#34;Action\u0026#34;: [ \u0026#34;ses:SendEmail\u0026#34;, \u0026#34;ses:SendRawEmail\u0026#34; ], \u0026#34;Resource\u0026#34;: \u0026#34;*\u0026#34; } ] } "},{"uri":"https://hd-2004.github.io/AWS_Fall2025_Workshop/5-workshop/","title":"Workshop","tags":[],"description":"","content":"English Journey Overview English Journey is an innovative web application designed to help users learn English vocabulary in a structured and interactive way. This platform leverages various AWS services to provide a smooth learning experience, allowing users to track their progress, engage with dynamic content, and receive personalized feedback.\nKey Features: User Authentication: Using Amazon Cognito, users can register, log in, and securely access their learning materials.\nInteractive Learning Modules: The app offers various interactive lessons to help users expand their vocabulary.\nProgress Tracking: Users can track their progress and completion of vocabulary lessons, with detailed reports generated through AWS Lambda and DynamoDB.\nNotifications: Email notifications via Amazon SES will inform users about new lessons, progress milestones, account updates, and other important changes.\nContent Storage: All learning materials are securely stored in Amazon S3 with proper access control.\nWeb Security: To protect the platform, AWS WAF ensures the application is protected from common web threats.\nMonitoring and Alerts: AWS CloudWatch is used to monitor platform performance, with alerts configured for potential issues.\nTechnologies Used: Frontend: Built using modern web technologies, ensuring a smooth and responsive user experience.\nBackend: Powered by AWS services like Lambda and DynamoDB, ensuring scalability and performance.\nStorage: All data and media content are securely stored in Amazon S3.\nContent Workshop overview Prerequiste Create Amplify AWS Cognito Configure Amazon SES for email CloudWatch IAM Roles - Policies Configure Amazon Route 53 Clean up "},{"uri":"https://hd-2004.github.io/AWS_Fall2025_Workshop/5-workshop/5.6-create-cloudwatch/","title":"Configure Amazon CloudWatch","tags":[],"description":"","content":"Goal To understand how the application behaves in production and to be able to react quickly to errors, we use Amazon CloudWatch for:\ncollecting logs from Lambda functions, monitoring metrics (invocations, errors, throttles, latency), 5.6.1 CloudWatch Logs for Lambda and API By default, every Lambda function created by Amplify writes logs to CloudWatch Logs.\nIn this project, logs are used to:\nTrack requests for: Level test submissions, Quiz submissions, Dictionary / vocabulary lookups, Debug input errors, permission (IAM) errors or timeouts, Record important application events. In addition to the default logs, some functions write structured JSON logs, which makes it easier to search by userId, requestId or feature.\n5.6.2 Metrics for key services CloudWatch automatically provides metrics for:\nLambda – invocations, errors, duration, concurrent executions, DynamoDB – read/write capacity, throttled requests, S3 / CloudFront – data transfer and request counts, WAF – number of allowed/blocked requests. For the workshop we focus on a few key metrics:\nLambda Error count and Error rate for the main backend functions, Lambda Duration to detect performance issues in level test \u0026amp; quiz processing, DynamoDB ThrottledRequests to see if the provisioned capacity is sufficient. 5.6.3 CloudWatch Dashboard (optional) To quickly observe the system health, the team creates a small CloudWatch Dashboard showing:\nA chart of Lambda error rate over time, Execution duration of the LevelTest and Dictionary functions, (Optional) Number of requests blocked by AWS WAF. The dashboard is not mandatory for the workshop, but it helps illustrate the system behavior when many students are using the application.\nSummary CloudWatch completes the monitoring story for English Journey by providing:\nLogs for analysis and debugging, Metrics \u0026amp; dashboards to observe trends, Combined with Amplify, SES and WAF, this gives a reasonably robust operational setup for this workshop project.\n"},{"uri":"https://hd-2004.github.io/AWS_Fall2025_Workshop/6-self-evaluation/","title":"Self-Assessment","tags":[],"description":"","content":" During my internship at [Company/Organization Name] from [start date] to [end date], I had the opportunity to learn, practice, and apply the knowledge acquired in school to a real-world working environment.\nI participated in [briefly describe the main project or task], through which I improved my skills in [list skills: programming, analysis, reporting, communication, etc.].\nIn terms of work ethic, I always strived to complete tasks well, complied with workplace regulations, and actively engaged with colleagues to improve work efficiency.\nTo objectively reflect on my internship period, I would like to evaluate myself based on the following criteria:\nNo. Criteria Description Good Fair Average 1 Professional knowledge \u0026amp; skills Understanding of the field, applying knowledge in practice, proficiency with tools, work quality ☐ ✅ ☐ 2 Ability to learn Ability to absorb new knowledge and learn quickly ☐ ✅ ☐ 3 Proactiveness Taking initiative, seeking out tasks without waiting for instructions ✅ ☐ ☐ 4 Sense of responsibility Completing tasks on time and ensuring quality ✅ ☐ ☐ 5 Discipline Adhering to schedules, rules, and work processes ✅ ☐ ☐ 6 Progressive mindset Willingness to receive feedback and improve oneself ☐ ✅ ☐ 7 Communication Presenting ideas and reporting work clearly ☐ ✅ ☐ 8 Teamwork Working effectively with colleagues and participating in teams ✅ ☐ ☐ 9 Professional conduct Respecting colleagues, partners, and the work environment ✅ ☐ ☐ 10 Problem-solving skills Identifying problems, proposing solutions, and showing creativity ☐ ✅ ☐ 11 Contribution to project/team Work effectiveness, innovative ideas, recognition from the team ✅ ☐ ☐ 12 Overall General evaluation of the entire internship period ☐ ✅ ☐ Needs Improvement Need to further enhance my knowledge base and effectively leverage tools to support my work Strive to improve my analytical thinking and problem-solving approach Aim to be more proactive in contributing ideas during team collaboration "},{"uri":"https://hd-2004.github.io/AWS_Fall2025_Workshop/5-workshop/5.7-create-iam-roles-policies/","title":"Create IAM Roles-Policies","tags":[],"description":"","content":"Goal This section explains how IAM roles and policies are designed for the English Journey application.\nMost roles are generated by AWS Amplify, but we still need to understand:\nwhich roles exist, what each role is allowed to do (S3, DynamoDB, SES, MediaConvert, …), and how we apply the least-privilege principle. 5.7.1 Overview of IAM in the architecture In the architecture from sections 5.3–5.6, IAM is the glue that connects services:\nAmplify uses IAM roles to deploy CloudFormation stacks and host the frontend. Lambda functions use execution roles to access DynamoDB, S3 and SES (for sending emails). CloudWatch and SES rely on IAM so alarms and email notifications can be sent correctly. The design goal is that each component only receives the minimum permissions it needs.\n5.7.2 Lambda execution roles When we define backend functions in Amplify (Level Test, My Learning, Dictionary, Vocabulary, …), Amplify automatically creates a Lambda execution role for each function.\nEach role has:\nTrust policy – allows the Lambda service to assume the role:\n{ \u0026#34;Effect\u0026#34;: \u0026#34;Allow\u0026#34;, \u0026#34;Principal\u0026#34;: { \u0026#34;Service\u0026#34;: \u0026#34;lambda.amazonaws.com\u0026#34; }, \u0026#34;Action\u0026#34;: \u0026#34;sts:AssumeRole\u0026#34; } "},{"uri":"https://hd-2004.github.io/AWS_Fall2025_Workshop/7-feedback/","title":"Feedback &amp; Suggestions","tags":[],"description":"","content":" Here you can freely share your personal opinions and experiences during the First Cloud Journey program to help the FCJ team improve based on the following criteria:\nOverall Evaluation 1. Working Environment\nThe working environment is welcoming and highly collaborative. FCJ team members are always ready to assist whenever I run into challenges — even beyond regular working hours. The workspace is well-organized and comfortable, which helps me stay focused and productive. Still, I believe that adding more team bonding activities or informal gatherings would help everyone connect and understand each other better.\n2. Support from Mentor / Admin Team\nMy mentor offers clear, thorough guidance and takes the time to explain concepts whenever I struggle to understand something. They also encourage me to ask questions and explore different approaches. The admin team is very supportive with documentation, administrative processes, and day-to-day logistics, making the internship experience seamless. I especially appreciate that my mentor allows me to experiment and solve problems independently rather than simply providing the answers.\n3. Alignment Between Work and Academic Background\nThe tasks assigned to me align well with what I have studied in school while also exposing me to new concepts and tools that I had not encountered before. This combination helped reinforce my academic foundation and develop practical, industry-relevant skills.\n4. Learning Opportunities \u0026amp; Skill Development\nDuring the internship, I gained valuable experience with project management tools, strengthened my teamwork abilities, and improved my professional communication in a corporate setting. My mentor also shared practical industry insights that helped me better understand and shape my career direction.\n5. Company Culture \u0026amp; Team Spirit\nThe company fosters a very positive culture where everyone is respectful, supportive, and dedicated. Despite the serious approach to work, the atmosphere remains friendly and energetic. When urgent tasks arise, the whole team collaborates effectively, offering help regardless of roles or seniority. This made me feel truly integrated into the team, even as an intern.\n6. Intern Policies / Benefits\nThe company provides internship allowances and accommodates flexible working hours when necessary. Moreover, being able to participate in internal training sessions is a significant advantage that greatly contributed to my professional growth.\nAdditional Questions What were you most satisfied with during the internship?\nI was most satisfied with the strong support from my mentor and the friendly, open working environment that helped me learn and grow quickly.\nWhat do you think the company should improve for future interns?\nI think it would be beneficial to introduce more mini-projects or specialized workshops so interns can gain more hands-on experience with real-world scenarios.\nWould you recommend this internship program to your friends? Why or why not?\nYes. Because it offers a great learning environment, supportive mentors, and opportunities to work with modern cloud technologies.\nSuggestions \u0026amp; Expectations Do you have any suggestions to improve the internship experience?\nI suggest adding more cross-team sharing sessions and offering multi-level training paths so interns can follow a clearer learning roadmap.\nWould you like to continue with this program in the future?\nYes, if given the opportunity, I would like to continue to deepen my knowledge and gain more practical experience.\nOther comments (free sharing):\nThank you to the FCJ team for your continuous support throughout my internship. I hope the program will keep growing and bring value to many more students in the future.\n"},{"uri":"https://hd-2004.github.io/AWS_Fall2025_Workshop/5-workshop/5.8-route-53/","title":"Configure Amazon Route 53","tags":[],"description":"","content":"5.8 Configure Amazon Route 53 (Custom Domain) In this step we connect the Amplify-hosted English Journey frontend to a custom domain managed by Amazon Route 53.\n🔗 Domain used in this workshop\nFor the demo environment we use the domain\nenglishjourney.xyz – the final site is available at:\nhttps://www.englishjourney.xyz/\n5.8.1 Create / verify the hosted zone Open the Route 53 console → Hosted zones → Create hosted zone. Enter your domain name, e.g. englishjourney.xyz, and keep type = Public hosted zone. Route 53 creates a set of NS and SOA records for the zone. If the domain is registered elsewhere, copy the Route 53 NS records to your registrar so that DNS is delegated to Route 53. 5.8.2 Connect the domain in AWS Amplify Go to the AWS Amplify console → select your English Journey app.\nIn the left menu choose Domain management → Add domain.\nSelect the hosted zone englishjourney.xyz.\nMap the root and sub-paths, for example:\nenglishjourney.xyz → main branch (production) www.englishjourney.xyz → redirect to root Amplify automatically creates the required A / AAAA and CNAME records in Route 53.\n5.8.3 Test the site Wait for DNS and SSL provisioning to complete (a few minutes).\nOpen a browser and navigate to:\nhttps://www.englishjourney.xyz/ Verify that the English Journey homepage is served correctly over HTTPS.\nNote this URL in your report / slides as the public entry point of the workshop application.\n"},{"uri":"https://hd-2004.github.io/AWS_Fall2025_Workshop/5-workshop/5.9-cleanup/","title":"Clean up","tags":[],"description":"","content":"Goal This section explains how to remove the AWS resources that were created or used during the English Journey workshop, so that you do not incur unexpected costs.\nYou should only delete these resources when you have finished experimenting with the architecture.\n5.9.1 – Delete the Amplify app and front-end hosting Open the Amplify console in the same Region used for the workshop. Select the Amplify app that is hosting the English Journey front end. Choose Actions → Delete app (or the Delete button in the app details page). Confirm the deletion as instructed. When you delete the Amplify app:\nAmplify automatically removes the front-end hosting, And usually deletes the backend stacks it created (Cognito, Lambda, DynamoDB),\nunless you explicitly choose to keep them during the deletion process. Carefully read the confirmation dialog to avoid deleting important resources by mistake.\n5.9.2 – Delete any remaining back-end resources Depending on how you created the back end, there may still be some resources left after deleting the Amplify app.\nIn the AWS console, in the workshop Region, check the following services:\nCognito\nDelete any User Pools or Identity Pools that were created specifically for the workshop. Lambda\nDelete Lambda functions that only serve English Journey (for example: level test handlers, daily reminders, vocabulary processing). DynamoDB\nDelete DynamoDB tables that were used only for workshop data (learning progress, questions, vocabulary, …) if you no longer need them. 5.9.3 – Clean up SES, CloudWatch and WAF In addition to the core backend, this workshop uses Amazon SES, CloudWatch and optionally AWS WAF.\nAmazon SES Open the Amazon SES console. In Verified identities: Delete email identities that were created only for the workshop (for example: test sender or test recipient addresses). In Configuration sets: Delete the configuration set used by the English Journey application (for example: english-journey-config), if you will not reuse it. If your account was moved out of SES sandbox only for the workshop, you may want to review your SES sending quotas and usage, but there is nothing extra to delete for that.\nCloudWatch Open the CloudWatch console. In Log groups, delete: Log groups for Lambda functions that belong to English Journey. AWS WAF If you deployed a dedicated WAF Web ACL for the English Journey frontend:\nOpen the AWS WAF console. Identify the Web ACL associated with the workshop CloudFront distribution or Amplify app. If the Web ACL is used exclusively for this workshop, delete it. 5.9.4 – Clean up IAM roles and policies Finally, review IAM to ensure there are no unused roles or policies left behind:\nIn the IAM console, go to Roles:\nLook for roles created only for this workshop (for example: custom Lambda execution roles, or roles with names that clearly reference English Journey or the workshop). Before deleting a role, confirm that no Lambda function, service or user still depends on it. In Policies:\nRemove customer-managed policies that were created solely for the workshop, especially: policies that grant ses:SendEmail / ses:SendRawEmail to Lambda, policies used only by temporary roles. Do not delete shared or production IAM roles / policies that might be reused by other applications.\nAfter these steps, the AWS environment should no longer contain resources that were created specifically for the English Journey workshop.\n"},{"uri":"https://hd-2004.github.io/AWS_Fall2025_Workshop/categories/","title":"Categories","tags":[],"description":"","content":""},{"uri":"https://hd-2004.github.io/AWS_Fall2025_Workshop/tags/","title":"Tags","tags":[],"description":"","content":""}]